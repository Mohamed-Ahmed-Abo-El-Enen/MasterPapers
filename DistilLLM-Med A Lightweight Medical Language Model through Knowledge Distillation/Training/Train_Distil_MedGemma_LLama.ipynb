{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rPIiRu5bazlp"
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers\n",
    "# !pip install -q bitsandbytes\n",
    "# !pip install -q datasets\n",
    "# !pip install -q lm-eval\n",
    "# !pip install -q wandb\n",
    "# !pip install -q ipywidgets\n",
    "# !pip install -q gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2OMs78goF4a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JJMhqmACoF4b"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.system('gdown --id 1qcKw1-vwtR4qMOB--B93b61mfD8JviKh')\n",
    "# os.system('gdown --id 1b9FWW2RSPT2Mdnz4YifNe7C-argn4KJJ')\n",
    "\n",
    "# os.system(\"unzip 'train_dataset_clean.zip'\")\n",
    "# os.system(\"unzip 'valid_dataset_clean.zip'\")\n",
    "\n",
    "# # os.makedirs(\"/root/.cache/huggingface/hub/dataset\", exist_ok=True)\n",
    "# # os.system(\"unzip 'train_dataset_clean.zip' -d '/root/.cache/huggingface/hub/dataset'\")\n",
    "# # os.system(\"unzip 'valid_dataset_clean.zip' -d '/root/.cache/huggingface/hub/dataset'\")\n",
    "\n",
    "# os.system(\"rm -rf 'train_dataset_clean.zip'\")\n",
    "# os.system(\"rm -rf 'valid_dataset_clean.zip'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcT0uco2oF4d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dEepIRL5bdN9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import (AutoModelForCausalLM, Gemma3ForConditionalGeneration, AutoProcessor,\n",
    "                          AutoTokenizer, BitsAndBytesConfig, Trainer, TrainingArguments)\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4Lpf2DKbnTR",
    "outputId": "b31d5de3-818b-4bdf-e90f-4af60e5af164"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/maboelenen/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamed-ahmed\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "# from google.colab import userdata\n",
    "\n",
    "HF_API_KEY = \"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\" # userdata.get('HUGGINGFACE_TOKEN')\n",
    "os.environ[\"HF_TOKEN\"] = HF_API_KEY\n",
    "WANDB_API_KEY =  \"2be7c86a28a2bcbeccdfa66844abfdd19b9bdabf\" # userdata.get('WANDB_API_KEY')\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "WANDB_PROJECT_NAME = \"Distil_MedGemma_LLama\"\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "if len(WANDB_PROJECT_NAME) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6QuEBumZ4f0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JXDxNyHCZ4cc"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9Xuj1-dRde3M"
   },
   "outputs": [],
   "source": [
    "teacher_model_name = \"google/medgemma-4b-it\"\n",
    "# student_model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "student_model_name = \"MohamedAhmedAE/distil_MedGemma_4B_Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H_NUYQYkkw_Y"
   },
   "outputs": [],
   "source": [
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_quant_type=\"nf4\",\n",
    "                                         bnb_4bit_compute_dtype=torch_dtype,\n",
    "                                         llm_int8_enable_fp32_cpu_offload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "8e40e8cbcb564c708df1d1715b909c00",
      "36ba5c6374e745b2b07dfad05cba40db",
      "e22503d4c08742cbb9cfd5d25a30b233",
      "c6bf564b97aa401f94040d371a47e2ce",
      "428a884a87594f60b9bcf7ff1094bc05",
      "ff061947057540759f3f8bc1b6ba6104",
      "2a904e6825f842dd877341c352e6f1b6",
      "4e890c81788a471e8c7d94516d92d7b5",
      "2044a45adac8461cbdc235d45b3f0e0c",
      "da459ff38df44f4eabae01ea2818d931",
      "372c9458a5a047dfab38be02a56510b9",
      "a61b6949e3e34a959eb7b8120d948eb9",
      "4c7815a99a6e42558531ba801d5d7d35",
      "dcab7bfb21f14b7b91b703040a335021",
      "c363a80fe6b8419493f7b764f0d03e9b",
      "85345328613b4b42b818c7b428981b9b",
      "43a1819a311946e9ae79aa1dc5f13e56",
      "f95e3297ad0e434ea9adb37b28d1e4dc",
      "ec82355504174bd09b0d1d755ab53896",
      "37c827ef3865422fbefdf8f41a303972",
      "bfaf04a018b541fb9e9bf09dcb3076cb",
      "7988fabd77c0491fb20f5595ad4d2cac",
      "1a2911ef854a4efeadf96c4bfd4a73dc",
      "73d75cb008dc457eb8b09c74b2da641e",
      "16d2051d46c0425d90f24dce4374a914",
      "af2eccec7e754b8dae5d7f8fd829956c",
      "83c203e3eaa44a1fb83c8446d0daae96",
      "414fb6fbd68143019547e8d3ee616c33",
      "0b080569994c4ae286441499a91e5ee3",
      "b3e251ce92554497818b5bbdf54b1d40",
      "91578453db2e47e484c2b0d585c8953c",
      "58a77a8bbd474aebac83abeb3939db44",
      "ea8df5685fb6480da5f41bdcbb7db06c",
      "5ab84b93475f441fb069428730590799",
      "a939ed4db8b840a98c5e8224261bed86",
      "784f48478b0243dbbc412af3c0c9b363",
      "b1925737c6d54441af545716f5cbb093",
      "1dfbe7ca0f0d4bf1a9b39e6d7aafc074",
      "95b6513e276249b08c338484cdd313e8",
      "af40090f58404571b5cdc9512a99e46f",
      "0ba921d0504f446796bc6f1c74b81af3",
      "f9b674528c8c4090a6bfd3e67cdff971",
      "14f7f6f34bba46c0b2d95f6461431f40",
      "c1f47c7d17284dc18210a1b2a2d67a3a",
      "031112a40abf4998bab80dba7c8971ac",
      "baaa40c876cb406bb1f7f3ec4ee3af24",
      "a6267e75a09341fd83d331f18b773b76",
      "4f0f7c597d2242d1888851e96bdcbdba",
      "0dcd4b74c62c4a48b6fd0dd2eed601e5",
      "47d0dd715ef8438dbec7572531979872",
      "625dd0485c034c7dbf33b7f6e92d2932",
      "066ac188099248a5992301c0fdaa95ec",
      "55ce78592c2e4e61b39cc93f5d4a2050",
      "7d08438e2d49438d81eb98ca782acd65",
      "14c93e79bc7c4d2b852a7e3846d71d26",
      "799e49fe61244565bc3b422a78ad589b"
     ]
    },
    "id": "kjhUfsRDef4E",
    "outputId": "9e204abb-7fd3-4e78-80bb-67dfa6ba90ad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bb0c2a60e245dab3aab6014c5c3fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a18d9755fc246c9b91de8b43c16199b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name,\n",
    "                                                               quantization_config=quantization_config,\n",
    "                                                               device_map=\"auto\",\n",
    "                                                               trust_remote_code=True,\n",
    "                                                               torch_dtype=torch_dtype,\n",
    "                                                               attn_implementation=attn_implementation)\n",
    "\n",
    "student_model = AutoModelForCausalLM.from_pretrained(student_model_name,\n",
    "                                                     torch_dtype=torch_dtype,\n",
    "                                                     device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-eAHbJZgabi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aHJsoxaVIZzN"
   },
   "outputs": [],
   "source": [
    "train_dataset_directory=\"content/train_dataset_clean\"\n",
    "valid_dataset_directory=\"content/valid_dataset_clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UxvCySJMOe-Q"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "\n",
    "train_dataset = Dataset.load_from_disk(train_dataset_directory)\n",
    "valid_dataset = Dataset.load_from_disk(valid_dataset_directory)\n",
    "\n",
    "formatted_dataset = concatenate_datasets([train_dataset, valid_dataset])\n",
    "formatted_dataset = formatted_dataset.shuffle(seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_dataset = formatted_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "78-tUlocIRBn"
   },
   "outputs": [],
   "source": [
    "del train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "63R-EQJ4oF43",
    "outputId": "6d711ca6-e49c-4062-c2da-1bf62132ea5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'instruction', 'input', 'context', 'choices', 'type', 'len_text', 'prompt'],\n",
       "    num_rows: 1437851\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWoWz0OloF42"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Pi0an59MoF44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "teacher_processor = AutoProcessor.from_pretrained(teacher_model_name)\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "\n",
    "if student_tokenizer.pad_token is None:\n",
    "    student_tokenizer.pad_token = student_tokenizer.eos_token\n",
    "    \n",
    "max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f576210eb34840a2915361f381a1ae22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1437851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading tokenizers...\")\n",
    "def format_prompt(example):\n",
    "    system = \"You are a Medical Assistant follow the following instruction\"\n",
    "\n",
    "    if len(example[\"context\"]) >= 5:\n",
    "        instruction = f\"\"\"{example[\"instruction\"]}\\n\\n{example[\"context\"]}\\n\\n\"\"\"\n",
    "\n",
    "    else:\n",
    "        instruction = f\"\"\"{example[\"instruction\"]}\\n\\n\"\"\"\n",
    "\n",
    "    if len(example[\"choices\"]) > 0:\n",
    "        prompt_template =\"\"\"{instruction}\n",
    "\n",
    "{input}\n",
    "\n",
    "{choices}\n",
    "\"\"\"\n",
    "        prompt = prompt_template.format(instruction=instruction.strip(),\n",
    "                                        input=example[\"input\"].strip(),\n",
    "                                        choices=example[\"choices\"].strip())\n",
    "    else:\n",
    "        prompt_template = \"\"\"{instruction}\n",
    "\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "        prompt = prompt_template.format(instruction=instruction.strip(),\n",
    "                                        input=example[\"input\"].strip())\n",
    "\n",
    "    student_message = [{\"role\": \"system\", \"content\": system},\n",
    "                       {\"role\": \"user\", \"content\": prompt},\n",
    "                       {\"role\": \"assistant\", \"content\": example['output'].strip()}]\n",
    "\n",
    "\n",
    "    teacher_message = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": system}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": example['output'].strip()}]\n",
    "    }]\n",
    "    \n",
    "    teacher_prompt = teacher_processor.apply_chat_template(teacher_message,\n",
    "                                                           tokenize=False,\n",
    "                                                           max_length=max_length,\n",
    "                                                           # add_generation_prompt=True,\n",
    "                                                           truncation=True)\n",
    "\n",
    "    student_prompt = student_tokenizer.apply_chat_template(student_message,\n",
    "                                                           tokenize=False,\n",
    "                                                           max_length=max_length,\n",
    "                                                           add_generation_prompt=True,\n",
    "                                                           truncation=True)\n",
    "\n",
    "    return {\"teacher_prompt\": teacher_prompt, \"student_prompt\": student_prompt}\n",
    "\n",
    "formatted_dataset = formatted_dataset.map(format_prompt)\n",
    "# formatted_dataset = formatted_dataset.select(range(100)).map(format_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BerBNR4_MYi6",
    "outputId": "ff77965f-d5d0-46a1-abf7-eea99677f2db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a Medical Assistant follow the following instruction\n",
      "\n",
      "Choose the correct answer for the following question\n",
      "\n",
      "Entonox cylinder has blue body and white shoulder.\n",
      "\n",
      "Colour of Entonox cylinder is\n",
      "\n",
      "1) Black body, white shoulder.\n",
      "2) Grey body, black and white shoulder.\n",
      "3) Black body, brown and white shoulder.\n",
      "4) Blue body, white shoulder.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "4<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(formatted_dataset['teacher_prompt'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 27 Jul 2025\n",
      "\n",
      "You are a Medical Assistant follow the following instruction<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Choose the correct answer for the following question\n",
      "\n",
      "Entonox cylinder has blue body and white shoulder.\n",
      "\n",
      "Colour of Entonox cylinder is\n",
      "\n",
      "1) Black body, white shoulder.\n",
      "2) Grey body, black and white shoulder.\n",
      "3) Black body, brown and white shoulder.\n",
      "4) Blue body, white shoulder.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "4<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(formatted_dataset['student_prompt'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnvHRpAdoF47"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "a2F473azHA1U"
   },
   "outputs": [],
   "source": [
    "def create_tokenize_function(teacher_processor, student_tokenizer, max_length=1024, \n",
    "                             student_text_column=\"student_prompt\", teacher_text_column=\"teacher_prompt\"):\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        \n",
    "        # Process teacher inputs\n",
    "        teacher_inputs = teacher_processor(\n",
    "            text=examples[teacher_text_column],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            padding_side=\"right\"\n",
    "        )\n",
    "\n",
    "        # Process student inputs\n",
    "        student_inputs = student_tokenizer(\n",
    "            examples[student_text_column],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            padding_side=\"right\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"teacher_input_ids\": teacher_inputs[\"input_ids\"].tolist(),\n",
    "            \"teacher_attention_mask\": teacher_inputs[\"attention_mask\"].tolist(),\n",
    "            \"teacher_labels\": teacher_inputs[\"input_ids\"].tolist(),\n",
    "            \"student_input_ids\": student_inputs[\"input_ids\"].tolist(),\n",
    "            \"student_attention_mask\": student_inputs[\"attention_mask\"].tolist(),\n",
    "            \"student_labels\": student_inputs[\"input_ids\"].tolist(),\n",
    "        }\n",
    "\n",
    "    return tokenize_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "530d29473e13460ca4cc8b94d7c04132",
      "483e273433e84a1092b77bba23cb43f0",
      "be7a21a92be94fff920203a26f39bd34",
      "f987acfdddf4450992adc3bda503d50a",
      "84a04c45efd947988c9fc7ee7e9e36a8",
      "3598ecd39cd54a92b66463cca4e5691a",
      "7f35a670402e41f88add53d14e2ba952",
      "8e7704ca610d490fbcfac4cbea6ef5db",
      "8442f54c1b524233bf63078b42957d9a",
      "19ff68e90b2e498ba6c6a617781254de",
      "81ce8fccc2c44faa9a25e26f1fc2c9c5",
      "6ece513f042747558dfddb49f70e98f3"
     ]
    },
    "id": "N15Tm14wHA4y",
    "outputId": "9eb73728-2870-44d6-df46-90863afd87fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset .... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4250a5512eb4974a31040d80f069a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing samples for distillation:   0%|          | 0/1437851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_function = create_tokenize_function(\n",
    "    teacher_processor=teacher_processor,\n",
    "    student_tokenizer=student_tokenizer,\n",
    "    max_length=max_length)\n",
    "\n",
    "print(\"Tokenizing dataset .... \")\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=formatted_dataset.column_names,\n",
    "    desc=\"Processing samples for distillation\",\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VzbieVwkqAv6"
   },
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "h0hkYX4Wii3r"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=1, ########################################\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    tokenized_dataset.select(range(100)),\n",
    "    batch_size=1, ########################################\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYb99GQ2qK5_"
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qfARX98FqCn9"
   },
   "outputs": [],
   "source": [
    "lr = 1e-6\n",
    "optimizer = AdamW(student_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rhc2WyymqPOi"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "FWma9UXmqNQc"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "temperature = 4.0\n",
    "alpha = 0.7\n",
    "accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in to HuggingFace Hub\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/maboelenen/Repos/ollama/wandb/run-20250727_152754-uf9zf7s6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mohamed-ahmed/distil_MedGemma_4B_Llama-3.2-1B/runs/uf9zf7s6' target=\"_blank\">distillation-3egin9y2</a></strong> to <a href='https://wandb.ai/mohamed-ahmed/distil_MedGemma_4B_Llama-3.2-1B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mohamed-ahmed/distil_MedGemma_4B_Llama-3.2-1B' target=\"_blank\">https://wandb.ai/mohamed-ahmed/distil_MedGemma_4B_Llama-3.2-1B</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mohamed-ahmed/distil_MedGemma_4B_Llama-3.2-1B/runs/uf9zf7s6' target=\"_blank\">https://wandb.ai/mohamed-ahmed/distil_MedGemma_4B_Llama-3.2-1B/runs/uf9zf7s6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gc  # For garbage collection\n",
    "from huggingface_hub import HfApi, Repository, login\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# HuggingFace configuration\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "HF_REPO_ID = \"MohamedAhmedAE/distil_MedGemma_4B_Llama-3.2-1B\"\n",
    "HF_PRIVATE = False\n",
    "\n",
    "# Login to HuggingFace Hub\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Logged in to HuggingFace Hub\")\n",
    "else:\n",
    "    print(\"Warning: HF_TOKEN not found. Set it as environment variable for auto-upload.\")\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"distil_MedGemma_4B_Llama-3.2-1B\",\n",
    "    name=f\"distillation-{wandb.util.generate_id()}\",\n",
    "    config={\n",
    "        \"alpha\": alpha,\n",
    "        \"temperature\": temperature,\n",
    "        \"accumulation_steps\": accumulation_steps,\n",
    "        \"hidden_dim\": 1024,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"teacher_vocab_size\": teacher_model.get_input_embeddings().weight.size(0),\n",
    "        \"student_vocab_size\": student_model.get_input_embeddings().weight.size(0),\n",
    "        \"save_every_steps\": 5000,\n",
    "        \"checkpoint_dir\": \"./checkpoints\",\n",
    "        \"hf_repo_id\": HF_REPO_ID,\n",
    "        \"hf_private\": HF_PRIVATE,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Setup checkpoint directory\n",
    "checkpoint_dir = Path(wandb.config.checkpoint_dir)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Memory optimization function\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressively clear GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get tokenizer for HuggingFace upload (assumes it's available)\n",
    "tokenizer = None\n",
    "try:\n",
    "    if hasattr(student_model, 'config') and hasattr(student_model.config, 'name_or_path'):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(student_model.config.name_or_path)\n",
    "except:\n",
    "    print(\"Warning: Could not load tokenizer. Model will be uploaded without tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.serialization import safe_globals\n",
    "import wandb.sdk.wandb_config\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename, model, teacher_proj, optimizer):\n",
    "    with safe_globals([wandb.sdk.wandb_config.Config]):\n",
    "        checkpoint = torch.load(filename, weights_only=True)\n",
    "    model.load_state_dict(checkpoint['student_state_dict'])\n",
    "    if teacher_proj is not None and checkpoint['teacher_proj_state_dict'] is not None:\n",
    "        teacher_proj.load_state_dict(checkpoint['teacher_proj_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return {\n",
    "        'start_step': checkpoint['start_step'],\n",
    "        'start_epoch': checkpoint['start_epoch']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enhanced Knowledge Distillation Training - Integrated Version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import wandb\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Enhanced hyperparameters - Add these to your wandb.config\n",
    "def get_enhanced_config_defaults():\n",
    "    \"\"\"Enhanced configuration defaults to add to your wandb.config\"\"\"\n",
    "    return {\n",
    "        'attention_distill': True,\n",
    "        'feature_matching': True,\n",
    "        'progressive_unfreezing': True,\n",
    "        'adaptive_temperature': True,\n",
    "        'layer_wise_distill': True,\n",
    "        'attention_loss_weight': 0.05,\n",
    "        'feature_loss_weight': 0.1,\n",
    "        'min_temperature': 1.0,\n",
    "        'max_temperature': 8.0,\n",
    "        'unfreeze_schedule': 'linear'  # 'linear', 'cosine', 'step'\n",
    "    }\n",
    "\n",
    "class EnhancedDistillationLoss(nn.Module):\n",
    "    \"\"\"Enhanced loss function with multiple distillation strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.cosine_loss = nn.CosineSimilarity(dim=-1)\n",
    "        \n",
    "    def compute_attention_loss(self, student_attentions, teacher_attentions):\n",
    "        \"\"\"Distill attention patterns\"\"\"\n",
    "        if not student_attentions or not teacher_attentions:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        try:\n",
    "            attention_loss = 0.0\n",
    "            min_layers = min(len(student_attentions), len(teacher_attentions))\n",
    "            \n",
    "            for i in range(min_layers):\n",
    "                # Average across heads for simplicity\n",
    "                s_att = student_attentions[i].mean(dim=1)  # [batch, seq, seq]\n",
    "                t_att = teacher_attentions[i].mean(dim=1)\n",
    "                \n",
    "                # Align sequence lengths\n",
    "                min_seq = min(s_att.size(-1), t_att.size(-1))\n",
    "                s_att = s_att[:, :min_seq, :min_seq]\n",
    "                t_att = t_att[:, :min_seq, :min_seq]\n",
    "                \n",
    "                # Check for NaN/Inf\n",
    "                if torch.isnan(s_att).any() or torch.isnan(t_att).any():\n",
    "                    continue\n",
    "                    \n",
    "                attention_loss += self.mse_loss(s_att, t_att)\n",
    "            \n",
    "            result = attention_loss / min_layers if min_layers > 0 else torch.tensor(0.0, device=self.device)\n",
    "            return result if not torch.isnan(result) else torch.tensor(0.0, device=self.device)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "    def compute_feature_matching_loss(self, student_hidden, teacher_hidden, intermediate_proj=None):\n",
    "        \"\"\"Feature matching across multiple layers\"\"\"\n",
    "        if not student_hidden or not teacher_hidden:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        try:\n",
    "            feature_loss = 0.0\n",
    "            num_layers = min(len(student_hidden), len(teacher_hidden))\n",
    "            \n",
    "            # Select layers to distill (every 2nd layer for efficiency)\n",
    "            layers_to_distill = list(range(0, num_layers, max(1, num_layers // 4)))\n",
    "            if not layers_to_distill:\n",
    "                layers_to_distill = [num_layers // 2]  # At least middle layer\n",
    "            \n",
    "            valid_layers = 0\n",
    "            \n",
    "            for layer_idx in layers_to_distill:\n",
    "                try:\n",
    "                    s_hidden = student_hidden[layer_idx]\n",
    "                    t_hidden = teacher_hidden[layer_idx]\n",
    "                    \n",
    "                    # Align sequence lengths\n",
    "                    min_seq = min(s_hidden.size(1), t_hidden.size(1))\n",
    "                    s_hidden = s_hidden[:, :min_seq, :]\n",
    "                    t_hidden = t_hidden[:, :min_seq, :]\n",
    "                    \n",
    "                    # Apply projection if needed for this layer\n",
    "                    if intermediate_proj is not None and s_hidden.size(-1) != t_hidden.size(-1):\n",
    "                        t_hidden = intermediate_proj(t_hidden)\n",
    "                    \n",
    "                    # Check for NaN/Inf\n",
    "                    if torch.isnan(s_hidden).any() or torch.isnan(t_hidden).any():\n",
    "                        continue\n",
    "                    \n",
    "                    # L2 distance loss\n",
    "                    layer_mse = self.mse_loss(s_hidden, t_hidden)\n",
    "                    if not torch.isnan(layer_mse):\n",
    "                        feature_loss += layer_mse\n",
    "                        valid_layers += 1\n",
    "                    \n",
    "                    # Cosine similarity loss (normalize hidden states)\n",
    "                    s_norm = F.normalize(s_hidden, p=2, dim=-1)\n",
    "                    t_norm = F.normalize(t_hidden, p=2, dim=-1)\n",
    "                    cosine_sim = self.cosine_loss(s_norm, t_norm).mean()\n",
    "                    \n",
    "                    if not torch.isnan(cosine_sim):\n",
    "                        feature_loss += (1 - cosine_sim)\n",
    "                        \n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            result = feature_loss / max(valid_layers, 1)\n",
    "            return result if not torch.isnan(result) else torch.tensor(0.0, device=self.device)\n",
    "            \n",
    "        except Exception:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "\n",
    "class AdaptiveTemperatureScheduler:\n",
    "    \"\"\"Adaptive temperature scheduling based on training progress\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_temp=4.0, min_temp=1.0, max_temp=8.0):\n",
    "        self.initial_temp = initial_temp\n",
    "        self.min_temp = min_temp\n",
    "        self.max_temp = max_temp\n",
    "        self.loss_history = []\n",
    "        self.current_temp = initial_temp\n",
    "        \n",
    "    def update(self, current_loss, global_step, total_steps):\n",
    "        try:\n",
    "            self.loss_history.append(current_loss)\n",
    "            \n",
    "            # Keep only recent history\n",
    "            if len(self.loss_history) > 100:\n",
    "                self.loss_history = self.loss_history[-100:]\n",
    "            \n",
    "            # Calculate loss trend\n",
    "            temp_adjustment = 1.0\n",
    "            if len(self.loss_history) >= 10:\n",
    "                recent_avg = np.mean(self.loss_history[-10:])\n",
    "                older_avg = np.mean(self.loss_history[-20:-10]) if len(self.loss_history) >= 20 else recent_avg\n",
    "                \n",
    "                # If loss is decreasing well, reduce temperature for sharper predictions\n",
    "                if recent_avg < older_avg * 0.95:\n",
    "                    temp_adjustment = 0.98\n",
    "                # If loss is stagnating, increase temperature for softer targets\n",
    "                elif recent_avg > older_avg * 1.02:\n",
    "                    temp_adjustment = 1.02\n",
    "            \n",
    "            # Base temperature decay\n",
    "            progress_ratio = global_step / max(total_steps, 1)\n",
    "            base_temp = self.initial_temp * (1 - 0.3 * progress_ratio)  # 30% reduction over training\n",
    "            \n",
    "            # Apply adjustment\n",
    "            self.current_temp = base_temp * temp_adjustment\n",
    "            self.current_temp = max(self.min_temp, min(self.max_temp, self.current_temp))\n",
    "            \n",
    "            return self.current_temp\n",
    "        except Exception:\n",
    "            return self.initial_temp\n",
    "\n",
    "class ProgressiveUnfreezer:\n",
    "    \"\"\"Progressive unfreezing strategy for student model\"\"\"\n",
    "    \n",
    "    def __init__(self, model, total_steps, unfreeze_schedule='linear'):\n",
    "        self.model = model\n",
    "        self.total_steps = total_steps\n",
    "        self.unfreeze_schedule = unfreeze_schedule\n",
    "        self.frozen_params = []\n",
    "        self.total_params = []\n",
    "        \n",
    "        # Initially freeze early layers\n",
    "        self.freeze_early_layers()\n",
    "    \n",
    "    def freeze_early_layers(self):\n",
    "        \"\"\"Freeze embedding and early transformer layers\"\"\"\n",
    "        try:\n",
    "            params_to_freeze = []\n",
    "            \n",
    "            # Collect all parameters first\n",
    "            for name, param in self.model.named_parameters():\n",
    "                self.total_params.append((name, param))\n",
    "            \n",
    "            # Freeze embeddings and early layers\n",
    "            for name, param in self.model.named_parameters():\n",
    "                should_freeze = False\n",
    "                \n",
    "                # Freeze embeddings\n",
    "                if any(embed_name in name.lower() for embed_name in ['embed', 'wte', 'wpe']):\n",
    "                    should_freeze = True\n",
    "                \n",
    "                # Freeze first few transformer layers\n",
    "                for layer_pattern, max_layers in [('layers.', 4), ('transformer.h.', 4), ('h.', 4)]:\n",
    "                    if layer_pattern in name:\n",
    "                        try:\n",
    "                            layer_num = int(name.split(layer_pattern)[1].split('.')[0])\n",
    "                            if layer_num < max_layers:\n",
    "                                should_freeze = True\n",
    "                        except (ValueError, IndexError):\n",
    "                            pass\n",
    "                \n",
    "                if should_freeze:\n",
    "                    param.requires_grad = False\n",
    "                    params_to_freeze.append((name, param))\n",
    "            \n",
    "            self.frozen_params = params_to_freeze\n",
    "            print(f\"Froze {len(self.frozen_params)} parameter groups for progressive unfreezing\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not setup progressive unfreezing: {e}\")\n",
    "            self.frozen_params = []\n",
    "    \n",
    "    def update_freezing(self, global_step):\n",
    "        \"\"\"Progressively unfreeze layers based on training progress\"\"\"\n",
    "        try:\n",
    "            if not self.frozen_params:\n",
    "                return\n",
    "            \n",
    "            progress = global_step / max(self.total_steps, 1)\n",
    "            \n",
    "            # Unfreeze schedule\n",
    "            if self.unfreeze_schedule == 'linear':\n",
    "                unfreeze_threshold = progress\n",
    "            elif self.unfreeze_schedule == 'cosine':\n",
    "                unfreeze_threshold = 0.5 * (1 - np.cos(np.pi * progress))\n",
    "            else:  # 'step'\n",
    "                unfreeze_threshold = 1.0 if progress > 0.5 else 0.0\n",
    "            \n",
    "            # Determine how many parameter groups to unfreeze\n",
    "            total_frozen = len(self.frozen_params)\n",
    "            params_to_unfreeze = int(total_frozen * unfreeze_threshold)\n",
    "            \n",
    "            # Unfreeze parameters\n",
    "            for i in range(min(params_to_unfreeze, total_frozen)):\n",
    "                name, param = self.frozen_params[i]\n",
    "                if not param.requires_grad:\n",
    "                    param.requires_grad = True\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in progressive unfreezing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze 37 parameter groups for progressive unfreezing\n",
      "Initialized projection layer: 262208 -> 128256\n",
      "Initialized intermediate projection: 2560 -> 2048\n",
      "Resumed training from step 305000, epoch 0\n",
      "Will skip 305000 batches in current epoch\n",
      "\n",
      "=== Starting Epoch 1/10 ===\n",
      "Resuming from global step 305000\n",
      "Skipping 305000 batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  21%|██████▎                       | 305000/1437851 [00:00<?, ?it/s]`sdpa` attention does not support `output_attentions=True` or `head_mask`. Please set your attention to `eager` if you want any of these features.\n",
      "Epoch 1/10:  22%|▏| 310000/1437851 [24:28<816:30:27,  2.61s/it, Loss=89.7247, Ha"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enhanced checkpoint at step 310000\n",
      "Epoch 1/10, Batch 310000, Step 310000\n",
      "  Total Loss: 89.7247\n",
      "  Hard Loss: 3.0000\n",
      "  Soft Loss: 1.7269\n",
      "  Attention Loss: 0.0000\n",
      "  Feature Loss: 872.0000\n",
      "  Current Alpha: 0.221\n",
      "  Current Temperature: 3.97\n",
      "  Learning Rate: 1.00e-06\n",
      "  Frozen Parameters: 37/37\n",
      "  GPU Memory: 12.11 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  22%|▏| 315000/1437851 [48:57<806:38:25,  2.59s/it, Loss=78.0299, Ha"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enhanced checkpoint at step 315000\n",
      "Epoch 1/10, Batch 315000, Step 315000\n",
      "  Total Loss: 78.0299\n",
      "  Hard Loss: 2.1094\n",
      "  Soft Loss: 1.7635\n",
      "  Attention Loss: 0.0000\n",
      "  Feature Loss: 760.0000\n",
      "  Current Alpha: 0.221\n",
      "  Current Temperature: 3.97\n",
      "  Learning Rate: 1.00e-06\n",
      "  Frozen Parameters: 37/37\n",
      "  GPU Memory: 12.11 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  22%|▏| 319153/1437851 [1:09:11<91:08:54,  3.41it/s, Loss=70.2340, H\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 230\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# === ENHANCED TEACHER MODEL INFERENCE ===\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     teacher_outputs = \u001b[43mteacher_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistill_intermediate_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeature_matching\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_distill\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m     teacher_logits = teacher_outputs.logits\n\u001b[32m    238\u001b[39m     teacher_hidden = teacher_outputs.hidden_states \u001b[38;5;28;01mif\u001b[39;00m (distill_intermediate_layers \u001b[38;5;129;01mor\u001b[39;00m feature_matching) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:1085\u001b[39m, in \u001b[36mGemma3ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\u001b[39m\n\u001b[32m   1080\u001b[39m output_hidden_states = (\n\u001b[32m   1081\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1082\u001b[39m )\n\u001b[32m   1083\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1102\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1103\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/transformers/utils/generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:939\u001b[39m, in \u001b[36mGemma3Model.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **lm_kwargs)\u001b[39m\n\u001b[32m    933\u001b[39m     \u001b[38;5;66;03m# Create the masks\u001b[39;00m\n\u001b[32m    934\u001b[39m     causal_mask_mapping = {\n\u001b[32m    935\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfull_attention\u001b[39m\u001b[33m\"\u001b[39m: create_causal_mask(**mask_kwargs),\n\u001b[32m    936\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msliding_attention\u001b[39m\u001b[33m\"\u001b[39m: create_sliding_window_causal_mask(**mask_kwargs),\n\u001b[32m    937\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m939\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Gemma3ModelOutputWithPast(\n\u001b[32m    953\u001b[39m     last_hidden_state=outputs.last_hidden_state,\n\u001b[32m    954\u001b[39m     past_key_values=outputs.past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    957\u001b[39m     image_hidden_states=image_features \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    958\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/transformers/utils/generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:566\u001b[39m, in \u001b[36mGemma3TextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    564\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    579\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/transformers/modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:385\u001b[39m, in \u001b[36mGemma3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    383\u001b[39m     position_embeddings = position_embeddings_global\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m    397\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py:313\u001b[39m, in \u001b[36mGemma3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    311\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    312\u001b[39m key_states = \u001b[38;5;28mself\u001b[39m.k_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m value_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    315\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_norm(query_states)\n\u001b[32m    316\u001b[39m key_states = \u001b[38;5;28mself\u001b[39m.k_norm(key_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:490\u001b[39m, in \u001b[36mLinear4bit.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    486\u001b[39m     x = x.to(\u001b[38;5;28mself\u001b[39m.compute_dtype)\n\u001b[32m    488\u001b[39m bias = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias.to(\u001b[38;5;28mself\u001b[39m.compute_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m.to(inp_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:393\u001b[39m, in \u001b[36mmatmul_4bit\u001b[39m\u001b[34m(A, B, quant_state, out, bias)\u001b[39m\n\u001b[32m    391\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/distil_train/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:322\u001b[39m, in \u001b[36mMatMul4Bit.forward\u001b[39m\u001b[34m(ctx, A, B, out, bias, quant_state)\u001b[39m\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.empty(A.shape[:-\u001b[32m1\u001b[39m] + B_shape[:\u001b[32m1\u001b[39m], dtype=A.dtype, device=A.device)\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[32m    325\u001b[39m ctx.state = quant_state\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Your existing code with enhancements integrated\n",
    "# Check if we should resume from a checkpoint\n",
    "teacher_proj = None\n",
    "student_vocab_size = student_model.get_input_embeddings().weight.size(0)\n",
    "teacher_vocab_size = teacher_model.get_input_embeddings().weight.size(0)\n",
    "hidden_dim = wandb.config.hidden_dim\n",
    "train_dataloader_len = len(train_dataloader)\n",
    "\n",
    "# Enhanced hyperparameters - merge with your existing config\n",
    "enhanced_defaults = get_enhanced_config_defaults()\n",
    "for key, default_value in enhanced_defaults.items():\n",
    "    if not hasattr(wandb.config, key):\n",
    "        setattr(wandb.config, key, default_value)\n",
    "\n",
    "temperature = wandb.config.get('temperature', 4.0)\n",
    "base_alpha = wandb.config.get('alpha', 0.7)\n",
    "distill_intermediate_layers = wandb.config.get('distill_intermediate', False)\n",
    "dynamic_alpha = wandb.config.get('dynamic_alpha', True)\n",
    "\n",
    "# New enhanced features\n",
    "attention_distill = wandb.config.get('attention_distill', True)\n",
    "feature_matching = wandb.config.get('feature_matching', True)\n",
    "progressive_unfreezing = wandb.config.get('progressive_unfreezing', True)\n",
    "adaptive_temperature = wandb.config.get('adaptive_temperature', True)\n",
    "attention_loss_weight = wandb.config.get('attention_loss_weight', 0.05)\n",
    "feature_loss_weight = wandb.config.get('feature_loss_weight', 0.1)\n",
    "\n",
    "# Calculate total training steps for dynamic scheduling\n",
    "total_steps = num_epochs * train_dataloader_len\n",
    "\n",
    "# Initialize enhanced components\n",
    "enhanced_loss = EnhancedDistillationLoss(wandb.config, device)\n",
    "temp_scheduler = AdaptiveTemperatureScheduler(\n",
    "    temperature, \n",
    "    wandb.config.get('min_temperature', 1.0),\n",
    "    wandb.config.get('max_temperature', 8.0)\n",
    ") if adaptive_temperature else None\n",
    "\n",
    "# Progressive unfreezing\n",
    "unfreezer = ProgressiveUnfreezer(\n",
    "    student_model, \n",
    "    total_steps, \n",
    "    wandb.config.get('unfreeze_schedule', 'linear')\n",
    ") if progressive_unfreezing else None\n",
    "\n",
    "# Enhanced learning rate scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer, \n",
    "    T_0=max(1, total_steps//4), \n",
    "    T_mult=2,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Your existing projection layer code\n",
    "if teacher_vocab_size != student_vocab_size:\n",
    "    teacher_proj = nn.Sequential(\n",
    "        nn.Linear(teacher_vocab_size, hidden_dim, bias=False, dtype=torch_dtype),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, student_vocab_size, bias=False, dtype=torch_dtype)\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nn.init.xavier_uniform_(teacher_proj[0].weight)\n",
    "        nn.init.xavier_uniform_(teacher_proj[2].weight)\n",
    "        teacher_proj[0].weight *= 0.1\n",
    "        teacher_proj[2].weight *= 0.1\n",
    "\n",
    "    print(f\"Initialized projection layer: {teacher_vocab_size} -> {student_vocab_size}\")\n",
    "    wandb.log({\"setup/projection_layer_created\": True})\n",
    "\n",
    "# Enhanced intermediate layer projection\n",
    "intermediate_proj = None\n",
    "if distill_intermediate_layers or feature_matching:\n",
    "    student_hidden_size = student_model.config.hidden_size\n",
    "    # teacher_hidden_size = teacher_model.config.hidden_size\n",
    "    teacher_hidden_size = teacher_model.config.text_config.hidden_size\n",
    "    \n",
    "    if student_hidden_size != teacher_hidden_size:\n",
    "        intermediate_proj = nn.Linear(\n",
    "            teacher_hidden_size, \n",
    "            student_hidden_size, \n",
    "            bias=False, \n",
    "            dtype=torch_dtype\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn.init.xavier_uniform_(intermediate_proj.weight)\n",
    "            intermediate_proj.weight *= 0.1\n",
    "        \n",
    "        print(f\"Initialized intermediate projection: {teacher_hidden_size} -> {student_hidden_size}\")\n",
    "\n",
    "# Your existing checkpoint configuration\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "final_checkpoint_path = os.path.join(checkpoint_dir, \"final_checkpoint.pt\")\n",
    "save_steps = wandb.config.save_every_steps\n",
    "\n",
    "# Your existing resume training logic\n",
    "start_step = 0\n",
    "start_epoch = 0\n",
    "batches_to_skip = 0\n",
    "\n",
    "if os.path.exists(final_checkpoint_path):\n",
    "    resume_info = load_checkpoint(final_checkpoint_path, student_model, teacher_proj, optimizer)\n",
    "    start_step = resume_info['start_step']\n",
    "    start_epoch = resume_info['start_epoch']\n",
    "    \n",
    "    batches_to_skip = start_step % train_dataloader_len\n",
    "    \n",
    "    print(f\"Resumed training from step {start_step}, epoch {start_epoch}\")\n",
    "    print(f\"Will skip {batches_to_skip} batches in current epoch\")\n",
    "else:\n",
    "    batches_to_skip = 0\n",
    "\n",
    "# Global step counter\n",
    "global_step = start_step\n",
    "\n",
    "# Enhanced training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f\"\\n=== Starting Epoch {epoch+1}/{num_epochs} ===\")\n",
    "    print(f\"Resuming from global step {global_step}\")\n",
    "    \n",
    "    # Model preparation\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "\n",
    "    # Initialize metrics\n",
    "    total_loss = 0.0\n",
    "    total_hard_loss = 0.0\n",
    "    total_soft_loss = 0.0\n",
    "    total_intermediate_loss = 0.0\n",
    "    total_attention_loss = 0.0  # New\n",
    "    total_feature_loss = 0.0    # New\n",
    "    num_valid_batches = 0\n",
    "    total_gradient_norm = 0.0\n",
    "    \n",
    "    # Enhanced error tracking\n",
    "    error_counts = {\n",
    "        \"data_processing\": 0,\n",
    "        \"teacher_inference\": 0,\n",
    "        \"student_inference\": 0,\n",
    "        \"projection\": 0,\n",
    "        \"loss_computation\": 0,\n",
    "        \"backward_pass\": 0,\n",
    "        \"nan_gradients\": 0,\n",
    "        \"nan_logits\": 0,\n",
    "        \"intermediate_distill\": 0,\n",
    "        \"attention_distill\": 0,  # New\n",
    "        \"feature_matching\": 0    # New\n",
    "    }\n",
    "\n",
    "    # Your existing dataloader setup\n",
    "    train_dataloader_iter = iter(train_dataloader)\n",
    "    \n",
    "    if epoch == start_epoch and batches_to_skip > 0:\n",
    "        print(f\"Skipping {batches_to_skip} batches...\")\n",
    "        for _ in range(batches_to_skip):\n",
    "            try:\n",
    "                next(train_dataloader_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        remaining_batches = train_dataloader_len - batches_to_skip\n",
    "        progress_bar = tqdm(\n",
    "            range(remaining_batches), \n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs}\", \n",
    "            initial=batches_to_skip, \n",
    "            total=train_dataloader_len\n",
    "        )\n",
    "        actual_batch_idx_offset = batches_to_skip\n",
    "    else:\n",
    "        remaining_batches = train_dataloader_len\n",
    "        progress_bar = tqdm(\n",
    "            range(train_dataloader_len), \n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs}\", \n",
    "            total=train_dataloader_len\n",
    "        )\n",
    "        actual_batch_idx_offset = 0\n",
    "\n",
    "    # Process remaining batches\n",
    "    for progress_idx in progress_bar:\n",
    "        actual_batch_idx = progress_idx + actual_batch_idx_offset if epoch == start_epoch else progress_idx\n",
    "        \n",
    "        try:\n",
    "            batch = next(train_dataloader_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "        global_step += 1\n",
    "        \n",
    "        # Update progressive unfreezing\n",
    "        if unfreezer:\n",
    "            unfreezer.update_freezing(global_step)\n",
    "        \n",
    "        # Dynamic alpha scheduling - start with more soft loss emphasis\n",
    "        if dynamic_alpha:\n",
    "            progress_ratio = global_step / total_steps\n",
    "            current_alpha = base_alpha * (0.3 + 0.7 * progress_ratio)\n",
    "        else:\n",
    "            current_alpha = base_alpha\n",
    "        \n",
    "        # Update adaptive temperature\n",
    "        current_temperature = temperature\n",
    "        if temp_scheduler:\n",
    "            # Use a simple proxy loss for temperature adaptation\n",
    "            try:\n",
    "                proxy_loss = total_loss / max(num_valid_batches, 1) if num_valid_batches > 0 else 1.0\n",
    "                current_temperature = temp_scheduler.update(proxy_loss, global_step, total_steps)\n",
    "            except:\n",
    "                current_temperature = temperature\n",
    "        \n",
    "        try:\n",
    "            # === DATA PROCESSING === (Your existing code)\n",
    "            teacher_input_ids = batch[\"teacher_input_ids\"].to(device, non_blocking=True)\n",
    "            teacher_attention_mask = batch[\"teacher_attention_mask\"].to(device, non_blocking=True)\n",
    "            teacher_labels = batch[\"teacher_labels\"].to(device, non_blocking=True)\n",
    "\n",
    "            student_input_ids = batch[\"student_input_ids\"].to(device, non_blocking=True)\n",
    "            student_attention_mask = batch[\"student_attention_mask\"].to(device, non_blocking=True)\n",
    "            student_labels = batch[\"student_labels\"].to(device, non_blocking=True)\n",
    "\n",
    "            # Validate data\n",
    "            if (torch.isnan(student_labels).any() or torch.isinf(student_labels).any() or \n",
    "                (student_labels != -100).sum() == 0):\n",
    "                error_counts[\"data_processing\"] += 1\n",
    "                continue\n",
    "\n",
    "            # === ENHANCED TEACHER MODEL INFERENCE ===\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(\n",
    "                    input_ids=teacher_input_ids,\n",
    "                    attention_mask=teacher_attention_mask,\n",
    "                    labels=teacher_labels,\n",
    "                    output_hidden_states=distill_intermediate_layers or feature_matching,\n",
    "                    output_attentions=attention_distill\n",
    "                )\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "                teacher_hidden = teacher_outputs.hidden_states if (distill_intermediate_layers or feature_matching) else None\n",
    "                teacher_attentions = teacher_outputs.attentions if attention_distill else None\n",
    "\n",
    "                if torch.isnan(teacher_logits).any():\n",
    "                    error_counts[\"teacher_inference\"] += 1\n",
    "                    continue\n",
    "\n",
    "            # Clear teacher inputs from memory\n",
    "            del teacher_input_ids, teacher_attention_mask, teacher_labels\n",
    "\n",
    "            # === ENHANCED STUDENT MODEL INFERENCE ===\n",
    "            student_outputs = student_model(\n",
    "                input_ids=student_input_ids,\n",
    "                attention_mask=student_attention_mask,\n",
    "                labels=student_labels,\n",
    "                output_hidden_states=distill_intermediate_layers or feature_matching,\n",
    "                output_attentions=attention_distill\n",
    "            )\n",
    "            student_logits = student_outputs.logits\n",
    "            student_hidden = student_outputs.hidden_states if (distill_intermediate_layers or feature_matching) else None\n",
    "            student_attentions = student_outputs.attentions if attention_distill else None\n",
    "\n",
    "            # Check for NaN/Inf in student outputs\n",
    "            if torch.isnan(student_logits).any() or torch.isinf(student_logits).any():\n",
    "                error_counts[\"nan_logits\"] += 1\n",
    "                print(f\"NaN/Inf in student logits at batch {actual_batch_idx}\")\n",
    "                \n",
    "                # Emergency parameter reinitialization\n",
    "                for name, module in student_model.named_modules():\n",
    "                    if hasattr(module, 'weight') and module.weight is not None:\n",
    "                        if torch.isnan(module.weight).any():\n",
    "                            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                    if hasattr(module, 'bias') and module.bias is not None:\n",
    "                        if torch.isnan(module.bias).any():\n",
    "                            nn.init.zeros_(module.bias)\n",
    "                continue\n",
    "\n",
    "            # Clear student inputs from memory\n",
    "            del student_input_ids, student_attention_mask\n",
    "\n",
    "            # === SEQUENCE LENGTH ALIGNMENT ===\n",
    "            min_seq_len = min(student_logits.size(1), teacher_logits.size(1))\n",
    "            student_logits = student_logits[:, :min_seq_len, :]\n",
    "            teacher_logits = teacher_logits[:, :min_seq_len, :]\n",
    "            student_labels_aligned = student_labels[:, :min_seq_len]\n",
    "\n",
    "            # === APPLY PROJECTION IF NEEDED ===\n",
    "            if teacher_proj is not None:\n",
    "                teacher_logits = teacher_proj(teacher_logits.to(torch_dtype))\n",
    "                if torch.isnan(teacher_logits).any():\n",
    "                    error_counts[\"projection\"] += 1\n",
    "                    continue\n",
    "\n",
    "            # === ENHANCED LOSS COMPUTATION ===\n",
    "            # Hard loss (student vs ground truth)\n",
    "            mask = (student_labels_aligned != -100)\n",
    "            if mask.sum() == 0:\n",
    "                error_counts[\"loss_computation\"] += 1\n",
    "                continue\n",
    "\n",
    "            flat_logits = student_logits.view(-1, student_logits.size(-1))\n",
    "            flat_labels = student_labels_aligned.view(-1)\n",
    "            hard_loss = F.cross_entropy(flat_logits, flat_labels, ignore_index=-100, reduction='mean')\n",
    "            \n",
    "            del flat_logits, flat_labels\n",
    "\n",
    "            if torch.isnan(hard_loss) or torch.isinf(hard_loss):\n",
    "                error_counts[\"loss_computation\"] += 1\n",
    "                continue\n",
    "\n",
    "            # Soft loss (Knowledge Distillation) with current temperature\n",
    "            mask_float = mask.float()\n",
    "            if mask_float.sum() == 0:\n",
    "                soft_loss = torch.tensor(0.0, device=device, dtype=torch_dtype)\n",
    "            else:\n",
    "                # Temperature scaling with stability checks\n",
    "                student_logits_temp = student_logits / current_temperature\n",
    "                teacher_logits_temp = teacher_logits / current_temperature\n",
    "\n",
    "                # Check for extreme values\n",
    "                if (torch.abs(student_logits_temp).max() > 50 or \n",
    "                    torch.abs(teacher_logits_temp).max() > 50):\n",
    "                    error_counts[\"loss_computation\"] += 1\n",
    "                    continue\n",
    "\n",
    "                student_log_probs = F.log_softmax(student_logits_temp, dim=-1)\n",
    "                teacher_probs = F.softmax(teacher_logits_temp, dim=-1)\n",
    "\n",
    "                if (torch.isnan(student_log_probs).any() or \n",
    "                    torch.isnan(teacher_probs).any()):\n",
    "                    error_counts[\"loss_computation\"] += 1\n",
    "                    continue\n",
    "\n",
    "                # KL divergence\n",
    "                kl_loss = F.kl_div(student_log_probs, teacher_probs, reduction='none', log_target=False)\n",
    "                kl_per_token = kl_loss.sum(-1)\n",
    "                masked_kl = kl_per_token * mask_float\n",
    "                soft_loss = masked_kl.sum() / mask_float.sum() * (current_temperature ** 2)\n",
    "\n",
    "                del (student_logits_temp, teacher_logits_temp, student_log_probs, \n",
    "                     teacher_probs, kl_loss, kl_per_token, masked_kl)\n",
    "\n",
    "            # Enhanced intermediate layer distillation\n",
    "            intermediate_loss = torch.tensor(0.0, device=device)\n",
    "            if distill_intermediate_layers and student_hidden is not None and teacher_hidden is not None:\n",
    "                try:\n",
    "                    # Use middle layer for distillation\n",
    "                    student_mid_layer = student_hidden[len(student_hidden) // 2][:, :min_seq_len, :]\n",
    "                    teacher_mid_layer = teacher_hidden[len(teacher_hidden) // 2][:, :min_seq_len, :]\n",
    "                    \n",
    "                    # Apply projection if needed\n",
    "                    if intermediate_proj is not None:\n",
    "                        teacher_mid_layer = intermediate_proj(teacher_mid_layer)\n",
    "                    \n",
    "                    # MSE loss for hidden states\n",
    "                    intermediate_loss = F.mse_loss(student_mid_layer, teacher_mid_layer, reduction='mean')\n",
    "                    \n",
    "                    if torch.isnan(intermediate_loss):\n",
    "                        intermediate_loss = torch.tensor(0.0, device=device)\n",
    "                        error_counts[\"intermediate_distill\"] += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    intermediate_loss = torch.tensor(0.0, device=device)\n",
    "                    error_counts[\"intermediate_distill\"] += 1\n",
    "\n",
    "            # NEW: Enhanced feature matching loss\n",
    "            feature_loss = torch.tensor(0.0, device=device)\n",
    "            if feature_matching and student_hidden is not None and teacher_hidden is not None:\n",
    "                try:\n",
    "                    feature_loss = enhanced_loss.compute_feature_matching_loss(\n",
    "                        student_hidden, teacher_hidden, intermediate_proj\n",
    "                    )\n",
    "                    if torch.isnan(feature_loss):\n",
    "                        feature_loss = torch.tensor(0.0, device=device)\n",
    "                        error_counts[\"feature_matching\"] += 1\n",
    "                except Exception:\n",
    "                    feature_loss = torch.tensor(0.0, device=device)\n",
    "                    error_counts[\"feature_matching\"] += 1\n",
    "\n",
    "            # NEW: Attention distillation loss\n",
    "            attention_loss = torch.tensor(0.0, device=device)\n",
    "            if attention_distill and student_attentions is not None and teacher_attentions is not None:\n",
    "                try:\n",
    "                    attention_loss = enhanced_loss.compute_attention_loss(\n",
    "                        student_attentions, teacher_attentions\n",
    "                    )\n",
    "                    if torch.isnan(attention_loss):\n",
    "                        attention_loss = torch.tensor(0.0, device=device)\n",
    "                        error_counts[\"attention_distill\"] += 1\n",
    "                except Exception:\n",
    "                    attention_loss = torch.tensor(0.0, device=device)\n",
    "                    error_counts[\"attention_distill\"] += 1\n",
    "\n",
    "            # Clear logits and other tensors\n",
    "            del student_logits, teacher_logits, student_labels, student_labels_aligned, mask, mask_float\n",
    "            if student_hidden is not None:\n",
    "                del student_hidden\n",
    "            if teacher_hidden is not None:\n",
    "                del teacher_hidden\n",
    "            if student_attentions is not None:\n",
    "                del student_attentions\n",
    "            if teacher_attentions is not None:\n",
    "                del teacher_attentions\n",
    "\n",
    "            if torch.isnan(soft_loss) or torch.isinf(soft_loss):\n",
    "                error_counts[\"loss_computation\"] += 1\n",
    "                continue\n",
    "\n",
    "            # Enhanced loss combination\n",
    "            total_batch_loss = (\n",
    "                current_alpha * soft_loss + \n",
    "                (1 - current_alpha) * hard_loss + \n",
    "                0.1 * intermediate_loss +  # Original intermediate loss\n",
    "                feature_loss_weight * feature_loss +  # New feature matching\n",
    "                attention_loss_weight * attention_loss  # New attention loss\n",
    "            )\n",
    "\n",
    "            # Apply gradient accumulation scaling BEFORE backward pass\n",
    "            if ((actual_batch_idx + 1) % accumulation_steps == 0) or (progress_idx == remaining_batches - 1):\n",
    "                pass\n",
    "            else:\n",
    "                total_batch_loss = total_batch_loss / accumulation_steps\n",
    "\n",
    "            if torch.isnan(total_batch_loss) or torch.isinf(total_batch_loss):\n",
    "                continue\n",
    "\n",
    "            # === BACKWARD PASS ===\n",
    "            total_batch_loss.backward()\n",
    "\n",
    "            # Check for NaN gradients\n",
    "            has_nan_grad = False\n",
    "            for param in student_model.parameters():\n",
    "                if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                    has_nan_grad = True\n",
    "                    break\n",
    "\n",
    "            if has_nan_grad:\n",
    "                optimizer.zero_grad()\n",
    "                error_counts[\"nan_gradients\"] += 1\n",
    "                continue\n",
    "\n",
    "            # Gradient clipping\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
    "            if teacher_proj is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(teacher_proj.parameters(), max_norm=1.0)\n",
    "            if intermediate_proj is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(intermediate_proj.parameters(), max_norm=1.0)\n",
    "\n",
    "            total_gradient_norm += grad_norm.item()\n",
    "\n",
    "            # === OPTIMIZATION ===\n",
    "            if ((actual_batch_idx + 1) % accumulation_steps == 0) or (progress_idx == remaining_batches - 1):\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Enhanced: Add scheduler step\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # === LOSS TRACKING ===\n",
    "            batch_total_loss_val = total_batch_loss.item() * (accumulation_steps if (actual_batch_idx + 1) % accumulation_steps != 0 else 1)\n",
    "            batch_hard_loss_val = hard_loss.item()\n",
    "            batch_soft_loss_val = soft_loss.item()\n",
    "            batch_intermediate_loss_val = intermediate_loss.item()\n",
    "            batch_attention_loss_val = attention_loss.item()  # New\n",
    "            batch_feature_loss_val = feature_loss.item()      # New\n",
    "\n",
    "            # Clear loss tensors\n",
    "            del total_batch_loss, hard_loss, soft_loss, intermediate_loss, attention_loss, feature_loss\n",
    "\n",
    "            # Accumulate losses\n",
    "            total_loss += batch_total_loss_val\n",
    "            total_hard_loss += batch_hard_loss_val\n",
    "            total_soft_loss += batch_soft_loss_val\n",
    "            total_intermediate_loss += batch_intermediate_loss_val\n",
    "            total_attention_loss += batch_attention_loss_val    # New\n",
    "            total_feature_loss += batch_feature_loss_val        # New\n",
    "            num_valid_batches += 1\n",
    "\n",
    "            # Update progress bar with enhanced metrics\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{batch_total_loss_val:.4f}',\n",
    "                'Hard': f'{batch_hard_loss_val:.4f}',\n",
    "                'Soft': f'{batch_soft_loss_val:.4f}',\n",
    "                'Attn': f'{batch_attention_loss_val:.4f}',\n",
    "                'Feat': f'{batch_feature_loss_val:.4f}',\n",
    "                'Alpha': f'{current_alpha:.3f}',\n",
    "                'Temp': f'{current_temperature:.2f}',\n",
    "                'Step': global_step\n",
    "            })\n",
    "\n",
    "            # === ENHANCED CHECKPOINT SAVING ===\n",
    "            if global_step % save_steps == 0 or (epoch == num_epochs - 1 and progress_idx == remaining_batches - 1):\n",
    "                # Enhanced checkpoint with new components\n",
    "                checkpoint_data = {\n",
    "                    'student_state_dict': student_model.state_dict(),\n",
    "                    'teacher_proj_state_dict': teacher_proj.state_dict() if teacher_proj is not None else None,\n",
    "                    'intermediate_proj_state_dict': intermediate_proj.state_dict() if intermediate_proj is not None else None,\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),  # New: Save scheduler state\n",
    "                    'start_step': global_step,\n",
    "                    'start_epoch': epoch,\n",
    "                    'config': wandb.config.as_dict(),\n",
    "                    'temperature_scheduler_state': {  # New: Temperature scheduler state\n",
    "                        'current_temp': current_temperature,\n",
    "                        'loss_history': temp_scheduler.loss_history if temp_scheduler else []\n",
    "                    } if temp_scheduler else None,\n",
    "                    'unfreezer_state': {  # New: Progressive unfreezer state\n",
    "                        'frozen_params': len(unfreezer.frozen_params) if unfreezer else 0\n",
    "                    } if unfreezer else None\n",
    "                }\n",
    "                \n",
    "                save_checkpoint(checkpoint_data, final_checkpoint_path)\n",
    "                print(f\"Saved enhanced checkpoint at step {global_step}\")\n",
    "\n",
    "            # === ENHANCED LOGGING ===\n",
    "            if (actual_batch_idx + 1) % save_steps == 0:\n",
    "                avg_grad_norm = total_gradient_norm / max(num_valid_batches, 1)\n",
    "                \n",
    "                # Enhanced batch metrics\n",
    "                batch_metrics = {\n",
    "                    \"batch/total_loss\": batch_total_loss_val,\n",
    "                    \"batch/hard_loss\": batch_hard_loss_val,\n",
    "                    \"batch/soft_loss\": batch_soft_loss_val,\n",
    "                    \"batch/intermediate_loss\": batch_intermediate_loss_val,\n",
    "                    \"batch/attention_loss\": batch_attention_loss_val,     # New\n",
    "                    \"batch/feature_loss\": batch_feature_loss_val,         # New\n",
    "                    \"batch/gradient_norm\": grad_norm.item(),\n",
    "                    \"batch/current_alpha\": current_alpha,\n",
    "                    \"batch/current_temperature\": current_temperature,     # New\n",
    "                    \"training/step\": global_step,\n",
    "                    \"training/epoch\": epoch,\n",
    "                    \"training/learning_rate\": scheduler.get_last_lr()[0], # New\n",
    "                    \"memory/gpu_usage_gb\": get_memory_usage(),\n",
    "                    \"hyperparams/temperature\": current_temperature,\n",
    "                    \"hyperparams/alpha\": current_alpha\n",
    "                }\n",
    "                \n",
    "                # Add progressive unfreezing metrics\n",
    "                if unfreezer:\n",
    "                    frozen_count = sum(1 for _, param in unfreezer.frozen_params if not param.requires_grad)\n",
    "                    total_frozen = len(unfreezer.frozen_params)\n",
    "                    batch_metrics[\"training/frozen_params_ratio\"] = frozen_count / max(total_frozen, 1)\n",
    "                    batch_metrics[\"training/unfrozen_params\"] = total_frozen - frozen_count\n",
    "                \n",
    "                wandb.log(batch_metrics)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {actual_batch_idx+1}, Step {global_step}\")\n",
    "                print(f\"  Total Loss: {batch_total_loss_val:.4f}\")\n",
    "                print(f\"  Hard Loss: {batch_hard_loss_val:.4f}\")\n",
    "                print(f\"  Soft Loss: {batch_soft_loss_val:.4f}\")\n",
    "                print(f\"  Attention Loss: {batch_attention_loss_val:.4f}\") # New\n",
    "                print(f\"  Feature Loss: {batch_feature_loss_val:.4f}\")     # New\n",
    "                if distill_intermediate_layers:\n",
    "                    print(f\"  Intermediate Loss: {batch_intermediate_loss_val:.4f}\")\n",
    "                print(f\"  Current Alpha: {current_alpha:.3f}\")\n",
    "                print(f\"  Current Temperature: {current_temperature:.2f}\")  # New\n",
    "                print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.2e}\") # New\n",
    "                if unfreezer:\n",
    "                    frozen_count = sum(1 for _, param in unfreezer.frozen_params if not param.requires_grad)\n",
    "                    print(f\"  Frozen Parameters: {frozen_count}/{len(unfreezer.frozen_params)}\")\n",
    "                print(f\"  GPU Memory: {get_memory_usage():.2f} GB\")\n",
    "\n",
    "            # === MEMORY CLEANUP ===\n",
    "            if (actual_batch_idx + 1) % 100 == 0:\n",
    "                clear_memory()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {actual_batch_idx}: {e}\")\n",
    "            optimizer.zero_grad()\n",
    "            clear_memory()\n",
    "            continue\n",
    "\n",
    "    # Close progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # === ENHANCED EPOCH SUMMARY ===\n",
    "    if num_valid_batches > 0:\n",
    "        avg_total_loss = total_loss / num_valid_batches\n",
    "        avg_hard_loss = total_hard_loss / num_valid_batches\n",
    "        avg_soft_loss = total_soft_loss / num_valid_batches\n",
    "        avg_intermediate_loss = total_intermediate_loss / num_valid_batches\n",
    "        avg_attention_loss = total_attention_loss / num_valid_batches      # New\n",
    "        avg_feature_loss = total_feature_loss / num_valid_batches          # New\n",
    "        avg_gradient_norm = total_gradient_norm / num_valid_batches\n",
    "    else:\n",
    "        avg_total_loss = avg_hard_loss = avg_soft_loss = avg_intermediate_loss = float('nan')\n",
    "        avg_attention_loss = avg_feature_loss = avg_gradient_norm = float('nan')\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
    "    print(f\"  Average Total Loss: {avg_total_loss:.4f}\")\n",
    "    print(f\"  Average Hard Loss: {avg_hard_loss:.4f}\")\n",
    "    print(f\"  Average Soft Loss: {avg_soft_loss:.4f}\")\n",
    "    print(f\"  Average Attention Loss: {avg_attention_loss:.4f}\")    # New\n",
    "    print(f\"  Average Feature Loss: {avg_feature_loss:.4f}\")        # New\n",
    "    if distill_intermediate_layers:\n",
    "        print(f\"  Average Intermediate Loss: {avg_intermediate_loss:.4f}\")\n",
    "    print(f\"  Final Alpha: {current_alpha:.3f}\")\n",
    "    print(f\"  Final Temperature: {current_temperature:.2f}\")         # New\n",
    "    print(f\"  Final Learning Rate: {scheduler.get_last_lr()[0]:.2e}\") # New\n",
    "    print(f\"  Valid Batches: {num_valid_batches}/{remaining_batches}\")\n",
    "    print(f\"  GPU Memory: {get_memory_usage():.2f} GB\")\n",
    "    \n",
    "    # Progressive unfreezing summary\n",
    "    if unfreezer:\n",
    "        frozen_count = sum(1 for _, param in unfreezer.frozen_params if not param.requires_grad)\n",
    "        unfrozen_count = len(unfreezer.frozen_params) - frozen_count\n",
    "        print(f\"  Progressive Unfreezing: {unfrozen_count}/{len(unfreezer.frozen_params)} parameters unfrozen\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Enhanced epoch metrics logging\n",
    "    epoch_metrics = {\n",
    "        \"epoch/total_loss\": avg_total_loss,\n",
    "        \"epoch/hard_loss\": avg_hard_loss,\n",
    "        \"epoch/soft_loss\": avg_soft_loss,\n",
    "        \"epoch/intermediate_loss\": avg_intermediate_loss,\n",
    "        \"epoch/attention_loss\": avg_attention_loss,        # New\n",
    "        \"epoch/feature_loss\": avg_feature_loss,            # New\n",
    "        \"epoch/gradient_norm\": avg_gradient_norm,\n",
    "        \"epoch/final_alpha\": current_alpha,\n",
    "        \"epoch/final_temperature\": current_temperature,    # New\n",
    "        \"epoch/final_learning_rate\": scheduler.get_last_lr()[0], # New\n",
    "        \"epoch/valid_batches\": num_valid_batches,\n",
    "        \"epoch/total_batches\": remaining_batches,\n",
    "        \"epoch/success_rate\": num_valid_batches / remaining_batches if remaining_batches > 0 else 0,\n",
    "        \"epoch/epoch_number\": epoch + 1,\n",
    "        \"training/global_step\": global_step,\n",
    "        \"memory/gpu_usage_gb\": get_memory_usage()\n",
    "    }\n",
    "\n",
    "    # Add progressive unfreezing metrics to epoch summary\n",
    "    if unfreezer:\n",
    "        frozen_count = sum(1 for _, param in unfreezer.frozen_params if not param.requires_grad)\n",
    "        total_frozen = len(unfreezer.frozen_params)\n",
    "        epoch_metrics[\"epoch/frozen_params_ratio\"] = frozen_count / max(total_frozen, 1)\n",
    "        epoch_metrics[\"epoch/unfrozen_params\"] = total_frozen - frozen_count\n",
    "        epoch_metrics[\"epoch/unfreezing_progress\"] = (total_frozen - frozen_count) / max(total_frozen, 1)\n",
    "\n",
    "    # Add error counts\n",
    "    for error_type, count in error_counts.items():\n",
    "        epoch_metrics[f\"errors/{error_type}\"] = count\n",
    "\n",
    "    # Add enhanced metrics totals\n",
    "    epoch_metrics[\"totals/attention_loss\"] = total_attention_loss\n",
    "    epoch_metrics[\"totals/feature_loss\"] = total_feature_loss\n",
    "\n",
    "    wandb.log(epoch_metrics)\n",
    "\n",
    "    # Clear memory at end of epoch\n",
    "    clear_memory()\n",
    "\n",
    "    # Reset batches_to_skip after first resumed epoch\n",
    "    batches_to_skip = 0\n",
    "\n",
    "print(\"Enhanced training completed successfully!\")\n",
    "\n",
    "# === FINAL MODEL SAVING ===\n",
    "print(\"\\nSaving final enhanced model...\")\n",
    "\n",
    "# Save the final student model\n",
    "final_model_path = os.path.join(checkpoint_dir, \"final_student_model\")\n",
    "student_model.save_pretrained(final_model_path)\n",
    "print(f\"Final student model saved to: {final_model_path}\")\n",
    "\n",
    "# Save additional components\n",
    "if teacher_proj is not None:\n",
    "    torch.save(teacher_proj.state_dict(), os.path.join(checkpoint_dir, \"teacher_projection.pt\"))\n",
    "    print(\"Teacher projection layer saved\")\n",
    "\n",
    "if intermediate_proj is not None:\n",
    "    torch.save(intermediate_proj.state_dict(), os.path.join(checkpoint_dir, \"intermediate_projection.pt\"))\n",
    "    print(\"Intermediate projection layer saved\")\n",
    "\n",
    "# Save training metadata\n",
    "training_metadata = {\n",
    "    \"total_steps\": global_step,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"final_temperature\": current_temperature if temp_scheduler else temperature,\n",
    "    \"final_alpha\": current_alpha,\n",
    "    \"enhanced_features_used\": {\n",
    "        \"attention_distillation\": attention_distill,\n",
    "        \"feature_matching\": feature_matching,\n",
    "        \"progressive_unfreezing\": progressive_unfreezing,\n",
    "        \"adaptive_temperature\": adaptive_temperature,\n",
    "        \"layer_wise_distillation\": distill_intermediate_layers\n",
    "    },\n",
    "    \"final_learning_rate\": scheduler.get_last_lr()[0],\n",
    "    \"config\": wandb.config.as_dict()\n",
    "}\n",
    "\n",
    "with open(os.path.join(checkpoint_dir, \"training_metadata.json\"), 'w') as f:\n",
    "    import json\n",
    "    json.dump(training_metadata, f, indent=2)\n",
    "\n",
    "print(\"Training metadata saved\")\n",
    "print(f\"All files saved in: {checkpoint_dir}\")\n",
    "\n",
    "# Final wandb summary\n",
    "wandb.log({\n",
    "    \"training/completed\": True,\n",
    "    \"training/total_steps\": global_step,\n",
    "    \"training/final_temperature\": current_temperature if temp_scheduler else temperature,\n",
    "    \"training/final_alpha\": current_alpha,\n",
    "    \"training/final_lr\": scheduler.get_last_lr()[0]\n",
    "})\n",
    "\n",
    "print(\"\\nEnhanced Knowledge Distillation Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maboelenen/.conda/envs/distil_train/lib/python3.12/site-packages/huggingface_hub/hf_api.py:9692: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93697b78e3b44f1a8b7e20cb1b4cd46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed checkpoint to HF Hub at step 319154\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Save model separately for HF\n",
    "    student_model.save_pretrained(os.path.join(checkpoint_dir, \"hf_model\"))\n",
    "    \n",
    "    # Create or update HF model card\n",
    "    with open(os.path.join(checkpoint_dir, \"hf_model\", \"README.md\"), \"w\") as f:\n",
    "        f.write(f\"Model checkpoint at step {global_step}\\n\")\n",
    "        f.write(f\"Training config: {wandb.config}\\n\")\n",
    "    \n",
    "    # Push to Hub\n",
    "    api = HfApi()\n",
    "    api.upload_folder(\n",
    "        folder_path=os.path.join(checkpoint_dir, \"hf_model\"),\n",
    "        repo_id=HF_REPO_ID,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=f\"Checkpoint at step {global_step}\"\n",
    "    )\n",
    "    print(f\"Pushed checkpoint to HF Hub at step {global_step}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to push to HF Hub: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mdistillation-3egin9y2\u001b[0m at: \u001b[34mhttps://wandb.ai/mohamed-ahmed/distil_MedGemma_4B_Llama-3.2-1B/runs/uf9zf7s6\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250727_152754-uf9zf7s6/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# student_model.push_to_hub(\"MohamedAhmedAE/distil_MedGemma_4B_Llama-3.2-1B\")\n",
    "# tokenizer.push_to_hub(\"MohamedAhmedAE/distil_MedGemma_4B_Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggressive_memory_clear():\n",
    "    \"\"\"\n",
    "    Aggressively clear CUDA memory and Python objects\n",
    "    Use this if you want maximum memory clearing before starting training\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    \n",
    "    print(\"Performing aggressive memory clearing...\")\n",
    "    \n",
    "    # Clear Python objects\n",
    "    gc.collect()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # Show initial memory\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            cached = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            print(f\"GPU {i} - Before clearing: {allocated:.2f}GB allocated, {cached:.2f}GB cached\")\n",
    "        \n",
    "        # Multiple rounds of clearing\n",
    "        for round_num in range(3):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Reset all memory statistics\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "        \n",
    "        # Final clearing\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Show final memory\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            cached = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            print(f\"GPU {i} - After clearing: {allocated:.2f}GB allocated, {cached:.2f}GB cached\")\n",
    "    \n",
    "    print(\"Aggressive memory clearing completed!\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "aggressive_memory_clear()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "031112a40abf4998bab80dba7c8971ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_baaa40c876cb406bb1f7f3ec4ee3af24",
       "IPY_MODEL_a6267e75a09341fd83d331f18b773b76",
       "IPY_MODEL_4f0f7c597d2242d1888851e96bdcbdba"
      ],
      "layout": "IPY_MODEL_0dcd4b74c62c4a48b6fd0dd2eed601e5"
     }
    },
    "042a5ce5470646c9a321825511d01c16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "066ac188099248a5992301c0fdaa95ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06a4fccfad3746f195f3ac7419c36e03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b080569994c4ae286441499a91e5ee3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ba921d0504f446796bc6f1c74b81af3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dcd4b74c62c4a48b6fd0dd2eed601e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14c93e79bc7c4d2b852a7e3846d71d26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14f7f6f34bba46c0b2d95f6461431f40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16d2051d46c0425d90f24dce4374a914": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3e251ce92554497818b5bbdf54b1d40",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_91578453db2e47e484c2b0d585c8953c",
      "value": 0
     }
    },
    "19ff68e90b2e498ba6c6a617781254de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a2911ef854a4efeadf96c4bfd4a73dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_73d75cb008dc457eb8b09c74b2da641e",
       "IPY_MODEL_16d2051d46c0425d90f24dce4374a914",
       "IPY_MODEL_af2eccec7e754b8dae5d7f8fd829956c"
      ],
      "layout": "IPY_MODEL_83c203e3eaa44a1fb83c8446d0daae96"
     }
    },
    "1dfbe7ca0f0d4bf1a9b39e6d7aafc074": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2044a45adac8461cbdc235d45b3f0e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2153bcccee604a59bf8a48979057b414": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "225ee39f7e584211a8adb2534f0b3a5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23210bc4e43f48da954836c08ca34bdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2344cb6803f74c158698420ee8705727": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24d066ea79664e259526332e08ca6b36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb41b06ad37444a6a765f6cd7f3bf2a5",
      "placeholder": "​",
      "style": "IPY_MODEL_283178bb9d5542a6990f8f8efc319852",
      "value": " 50.5k/50.5k [00:00&lt;00:00, 3.09MB/s]"
     }
    },
    "2535fbe973da4ef89f78a430e902a094": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b54f3e0141c843be801e35c8cae09d02",
       "IPY_MODEL_e074170ebd5049669694a5fd580581be",
       "IPY_MODEL_f4630ad5252840b697f4c27f54d0c94a"
      ],
      "layout": "IPY_MODEL_85bca4f827ed41a2a0d101655d8d24a4"
     }
    },
    "283178bb9d5542a6990f8f8efc319852": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a904e6825f842dd877341c352e6f1b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c66e1d3d4d84563874b4c1eee364b6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3120dee3f72c4a7ea6e33a98a4467105": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3598ecd39cd54a92b66463cca4e5691a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36ba5c6374e745b2b07dfad05cba40db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff061947057540759f3f8bc1b6ba6104",
      "placeholder": "​",
      "style": "IPY_MODEL_2a904e6825f842dd877341c352e6f1b6",
      "value": "config.json: 100%"
     }
    },
    "372c9458a5a047dfab38be02a56510b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37c827ef3865422fbefdf8f41a303972": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3b20d2804f05477391c6f5cb6aa14816": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b6aa7cb70f045b7ae21739b7e1bc236",
      "placeholder": "​",
      "style": "IPY_MODEL_9a0c7815e03342d19b7aad518519d9e9",
      "value": "tokenizer.json: 100%"
     }
    },
    "3d81f8fce6f44f3fb270476822925495": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3fd3b9053bf34f7e81039d20bd25dcdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcdd9d0ff79c48f193b97b1f93f3b377",
      "placeholder": "​",
      "style": "IPY_MODEL_225ee39f7e584211a8adb2534f0b3a5b",
      "value": " 9.09M/9.09M [00:00&lt;00:00, 19.6MB/s]"
     }
    },
    "414fb6fbd68143019547e8d3ee616c33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "428a884a87594f60b9bcf7ff1094bc05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43a1819a311946e9ae79aa1dc5f13e56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47d0dd715ef8438dbec7572531979872": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "483e273433e84a1092b77bba23cb43f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3598ecd39cd54a92b66463cca4e5691a",
      "placeholder": "​",
      "style": "IPY_MODEL_7f35a670402e41f88add53d14e2ba952",
      "value": "Processing samples for distillation: 100%"
     }
    },
    "4c7815a99a6e42558531ba801d5d7d35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43a1819a311946e9ae79aa1dc5f13e56",
      "placeholder": "​",
      "style": "IPY_MODEL_f95e3297ad0e434ea9adb37b28d1e4dc",
      "value": "model.safetensors.index.json: 100%"
     }
    },
    "4e890c81788a471e8c7d94516d92d7b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f0f7c597d2242d1888851e96bdcbdba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d08438e2d49438d81eb98ca782acd65",
      "placeholder": "​",
      "style": "IPY_MODEL_14c93e79bc7c4d2b852a7e3846d71d26",
      "value": " 2.50G/3.64G [11:43&lt;03:22, 5.63MB/s]"
     }
    },
    "500c10186b784fe3a04838cac78b7faa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ace6c3699344d5cb010f0a2a318551b",
      "max": 50500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2c66e1d3d4d84563874b4c1eee364b6f",
      "value": 50500
     }
    },
    "530d29473e13460ca4cc8b94d7c04132": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_483e273433e84a1092b77bba23cb43f0",
       "IPY_MODEL_be7a21a92be94fff920203a26f39bd34",
       "IPY_MODEL_f987acfdddf4450992adc3bda503d50a"
      ],
      "layout": "IPY_MODEL_84a04c45efd947988c9fc7ee7e9e36a8"
     }
    },
    "55ce78592c2e4e61b39cc93f5d4a2050": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "58a77a8bbd474aebac83abeb3939db44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ab84b93475f441fb069428730590799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a939ed4db8b840a98c5e8224261bed86",
       "IPY_MODEL_784f48478b0243dbbc412af3c0c9b363",
       "IPY_MODEL_b1925737c6d54441af545716f5cbb093"
      ],
      "layout": "IPY_MODEL_1dfbe7ca0f0d4bf1a9b39e6d7aafc074"
     }
    },
    "5fdbdd80203d43bf852e8b1f94c8858e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "625dd0485c034c7dbf33b7f6e92d2932": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68d80d6ba4e241b388e238bf99a75687": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b6aa7cb70f045b7ae21739b7e1bc236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73d75cb008dc457eb8b09c74b2da641e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_414fb6fbd68143019547e8d3ee616c33",
      "placeholder": "​",
      "style": "IPY_MODEL_0b080569994c4ae286441499a91e5ee3",
      "value": "Fetching 2 files:   0%"
     }
    },
    "784f48478b0243dbbc412af3c0c9b363": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ba921d0504f446796bc6f1c74b81af3",
      "max": 4961251752,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f9b674528c8c4090a6bfd3e67cdff971",
      "value": 3019437700
     }
    },
    "7988fabd77c0491fb20f5595ad4d2cac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d08438e2d49438d81eb98ca782acd65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f35a670402e41f88add53d14e2ba952": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "800ad3af810b4e70bfce548a779c20aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81ce8fccc2c44faa9a25e26f1fc2c9c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83c203e3eaa44a1fb83c8446d0daae96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8442f54c1b524233bf63078b42957d9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "84a04c45efd947988c9fc7ee7e9e36a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85345328613b4b42b818c7b428981b9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85bca4f827ed41a2a0d101655d8d24a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88576ffe3bb44af58d13bac19c7133c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8ace6c3699344d5cb010f0a2a318551b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e40e8cbcb564c708df1d1715b909c00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_36ba5c6374e745b2b07dfad05cba40db",
       "IPY_MODEL_e22503d4c08742cbb9cfd5d25a30b233",
       "IPY_MODEL_c6bf564b97aa401f94040d371a47e2ce"
      ],
      "layout": "IPY_MODEL_428a884a87594f60b9bcf7ff1094bc05"
     }
    },
    "8e7704ca610d490fbcfac4cbea6ef5db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f39eb3e76394d0da0beb1c7a96bc67a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91578453db2e47e484c2b0d585c8953c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "95b6513e276249b08c338484cdd313e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a0c7815e03342d19b7aad518519d9e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a61b6949e3e34a959eb7b8120d948eb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c7815a99a6e42558531ba801d5d7d35",
       "IPY_MODEL_dcab7bfb21f14b7b91b703040a335021",
       "IPY_MODEL_c363a80fe6b8419493f7b764f0d03e9b"
      ],
      "layout": "IPY_MODEL_85345328613b4b42b818c7b428981b9b"
     }
    },
    "a6267e75a09341fd83d331f18b773b76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_066ac188099248a5992301c0fdaa95ec",
      "max": 3639026128,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55ce78592c2e4e61b39cc93f5d4a2050",
      "value": 2498855255
     }
    },
    "a939ed4db8b840a98c5e8224261bed86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95b6513e276249b08c338484cdd313e8",
      "placeholder": "​",
      "style": "IPY_MODEL_af40090f58404571b5cdc9512a99e46f",
      "value": "model-00001-of-00002.safetensors:  61%"
     }
    },
    "af2eccec7e754b8dae5d7f8fd829956c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58a77a8bbd474aebac83abeb3939db44",
      "placeholder": "​",
      "style": "IPY_MODEL_ea8df5685fb6480da5f41bdcbb7db06c",
      "value": " 0/2 [00:00&lt;?, ?it/s]"
     }
    },
    "af40090f58404571b5cdc9512a99e46f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1925737c6d54441af545716f5cbb093": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14f7f6f34bba46c0b2d95f6461431f40",
      "placeholder": "​",
      "style": "IPY_MODEL_c1f47c7d17284dc18210a1b2a2d67a3a",
      "value": " 3.02G/4.96G [11:33&lt;12:45, 2.54MB/s]"
     }
    },
    "b3e251ce92554497818b5bbdf54b1d40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b54f3e0141c843be801e35c8cae09d02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68d80d6ba4e241b388e238bf99a75687",
      "placeholder": "​",
      "style": "IPY_MODEL_8f39eb3e76394d0da0beb1c7a96bc67a",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "baaa40c876cb406bb1f7f3ec4ee3af24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47d0dd715ef8438dbec7572531979872",
      "placeholder": "​",
      "style": "IPY_MODEL_625dd0485c034c7dbf33b7f6e92d2932",
      "value": "model-00002-of-00002.safetensors:  69%"
     }
    },
    "bb41b06ad37444a6a765f6cd7f3bf2a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be7a21a92be94fff920203a26f39bd34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e7704ca610d490fbcfac4cbea6ef5db",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8442f54c1b524233bf63078b42957d9a",
      "value": 1000
     }
    },
    "bfaf04a018b541fb9e9bf09dcb3076cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1f47c7d17284dc18210a1b2a2d67a3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c363a80fe6b8419493f7b764f0d03e9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bfaf04a018b541fb9e9bf09dcb3076cb",
      "placeholder": "​",
      "style": "IPY_MODEL_7988fabd77c0491fb20f5595ad4d2cac",
      "value": " 90.6k/90.6k [00:00&lt;00:00, 1.39MB/s]"
     }
    },
    "c3e7e42425ed4b7ca6bcc34d40e8b684": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2153bcccee604a59bf8a48979057b414",
      "max": 9085657,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3d81f8fce6f44f3fb270476822925495",
      "value": 9085657
     }
    },
    "c6bf564b97aa401f94040d371a47e2ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da459ff38df44f4eabae01ea2818d931",
      "placeholder": "​",
      "style": "IPY_MODEL_372c9458a5a047dfab38be02a56510b9",
      "value": " 2.47k/2.47k [00:00&lt;00:00, 253kB/s]"
     }
    },
    "da459ff38df44f4eabae01ea2818d931": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcab7bfb21f14b7b91b703040a335021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec82355504174bd09b0d1d755ab53896",
      "max": 90594,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_37c827ef3865422fbefdf8f41a303972",
      "value": 90594
     }
    },
    "e074170ebd5049669694a5fd580581be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_800ad3af810b4e70bfce548a779c20aa",
      "max": 301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88576ffe3bb44af58d13bac19c7133c9",
      "value": 301
     }
    },
    "e22503d4c08742cbb9cfd5d25a30b233": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e890c81788a471e8c7d94516d92d7b5",
      "max": 2469,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2044a45adac8461cbdc235d45b3f0e0c",
      "value": 2469
     }
    },
    "ea8df5685fb6480da5f41bdcbb7db06c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec82355504174bd09b0d1d755ab53896": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee1fab7771fa402084ff9cc1c8e760c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ffd80e1005e54e3ba8869abbce6067a3",
       "IPY_MODEL_500c10186b784fe3a04838cac78b7faa",
       "IPY_MODEL_24d066ea79664e259526332e08ca6b36"
      ],
      "layout": "IPY_MODEL_5fdbdd80203d43bf852e8b1f94c8858e"
     }
    },
    "f4630ad5252840b697f4c27f54d0c94a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2344cb6803f74c158698420ee8705727",
      "placeholder": "​",
      "style": "IPY_MODEL_06a4fccfad3746f195f3ac7419c36e03",
      "value": " 301/301 [00:00&lt;00:00, 23.8kB/s]"
     }
    },
    "f95e3297ad0e434ea9adb37b28d1e4dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f987acfdddf4450992adc3bda503d50a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19ff68e90b2e498ba6c6a617781254de",
      "placeholder": "​",
      "style": "IPY_MODEL_81ce8fccc2c44faa9a25e26f1fc2c9c5",
      "value": " 1000/1000 [00:02&lt;00:00, 290.13 examples/s]"
     }
    },
    "f9b674528c8c4090a6bfd3e67cdff971": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb23f4f1ac92407c895beb89e5405f82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3b20d2804f05477391c6f5cb6aa14816",
       "IPY_MODEL_c3e7e42425ed4b7ca6bcc34d40e8b684",
       "IPY_MODEL_3fd3b9053bf34f7e81039d20bd25dcdb"
      ],
      "layout": "IPY_MODEL_23210bc4e43f48da954836c08ca34bdd"
     }
    },
    "fcdd9d0ff79c48f193b97b1f93f3b377": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff061947057540759f3f8bc1b6ba6104": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffd80e1005e54e3ba8869abbce6067a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3120dee3f72c4a7ea6e33a98a4467105",
      "placeholder": "​",
      "style": "IPY_MODEL_042a5ce5470646c9a321825511d01c16",
      "value": "tokenizer_config.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
