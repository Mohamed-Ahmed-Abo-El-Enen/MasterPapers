{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPIiRu5bazlp",
    "outputId": "66f49381-adc3-435f-ceca-81d412dbef0b"
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers\n",
    "# !pip install -q bitsandbytes\n",
    "# !pip install -q datasets\n",
    "# !pip install -q lm-eval\n",
    "# !pip install -q wandb\n",
    "# !pip install -q ipywidgets\n",
    "# !pip install -q gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2OMs78goF4a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJMhqmACoF4b",
    "outputId": "7e983068-baa2-4a74-a77c-a683615168d1"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.system('gdown --id 1qcKw1-vwtR4qMOB--B93b61mfD8JviKh')\n",
    "# os.system('gdown --id 1b9FWW2RSPT2Mdnz4YifNe7C-argn4KJJ')\n",
    "\n",
    "# os.system(\"unzip 'train_dataset_clean.zip'\")\n",
    "# os.system(\"unzip 'valid_dataset_clean.zip'\")\n",
    "\n",
    "# # os.makedirs(\"/root/.cache/huggingface/hub/dataset\", exist_ok=True)\n",
    "# # os.system(\"unzip 'train_dataset_clean.zip' -d '/root/.cache/huggingface/hub/dataset'\")\n",
    "# # os.system(\"unzip 'valid_dataset_clean.zip' -d '/root/.cache/huggingface/hub/dataset'\")\n",
    "\n",
    "# os.system(\"rm -rf 'train_dataset_clean.zip'\")\n",
    "# os.system(\"rm -rf 'valid_dataset_clean.zip'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcT0uco2oF4d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DNfVNQ43I3E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhfD1L343JAB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4Lpf2DKbnTR",
    "outputId": "835c7ebd-360f-4581-9cbf-5e3721ca9e4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/maboelenen/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamed-ahmed\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "# from google.colab import userdata\n",
    "\n",
    "HF_API_KEY = \"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\" # userdata.get('HUGGINGFACE_TOKEN')\n",
    "WANDB_API_KEY =  \"2be7c86a28a2bcbeccdfa66844abfdd19b9bdabf\" # userdata.get('WANDB_API_KEY')\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"HF_TOKEN\"] = HF_API_KEY\n",
    "\n",
    "WANDB_PROJECT_NAME = \"distil_med42_8B_Llama-3.2-1B-Instruct\"\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "if len(WANDB_PROJECT_NAME) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T6QuEBumZ4f0"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\"MohamedAhmedAE/distil_med42_8B_Llama-3.2-1B-Instruct\", local_dir=\"./distil_med42_8B_Llama-3.2-1B-Instruct\", local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXDxNyHCZ4cc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Xuj1-dRde3M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aHJsoxaVIZzN"
   },
   "outputs": [],
   "source": [
    "train_dataset_directory=\"content/train_dataset_clean\"\n",
    "valid_dataset_directory=\"content/valid_dataset_clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UxvCySJMOe-Q"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "\n",
    "train_dataset = Dataset.load_from_disk(train_dataset_directory)\n",
    "valid_dataset = Dataset.load_from_disk(valid_dataset_directory)\n",
    "\n",
    "# train_dataset = train_dataset.select(range(1000))\n",
    "# valid_dataset = valid_dataset.select(range(1000))\n",
    "\n",
    "formatted_dataset = concatenate_datasets([train_dataset, valid_dataset])\n",
    "formatted_dataset = formatted_dataset.shuffle(seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "78-tUlocIRBn"
   },
   "outputs": [],
   "source": [
    "del train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "63R-EQJ4oF43"
   },
   "outputs": [],
   "source": [
    "# formatted_dataset = formatted_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWoWz0OloF42"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pi0an59MoF44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "alfUDKtyVlNS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizers...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "student_ckpt = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "max_length=1024\n",
    "\n",
    "\n",
    "print(\"Loading tokenizers...\")\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
    "if student_tokenizer.pad_token is None:\n",
    "    student_tokenizer.pad_token = student_tokenizer.eos_token\n",
    "\n",
    "\n",
    "def format_prompt(example):\n",
    "    system = \"You are a Medical Assistant follow the following instruction\"\n",
    "\n",
    "    if len(example[\"context\"]) >= 5:\n",
    "        instruction = f\"\"\"{example[\"instruction\"]}\\n\\n{example[\"context\"]}\\n\\n\"\"\"\n",
    "\n",
    "    else:\n",
    "        instruction = f\"\"\"{example[\"instruction\"]}\\n\\n\"\"\"\n",
    "\n",
    "    if len(example[\"choices\"]) > 0:\n",
    "        prompt_template =\"\"\"{instruction}\n",
    "\n",
    "{input}\n",
    "\n",
    "{choices}\n",
    "\"\"\"\n",
    "        prompt = prompt_template.format(instruction=instruction.strip(),\n",
    "                                        input=example[\"input\"].strip(),\n",
    "                                        choices=example[\"choices\"].strip())\n",
    "    else:\n",
    "        prompt_template = \"\"\"{instruction}\n",
    "\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "        prompt = prompt_template.format(instruction=instruction.strip(),\n",
    "                                        input=example[\"input\"].strip())\n",
    "\n",
    "    message = [{\"role\": \"system\", \"content\": system},\n",
    "               {\"role\": \"user\", \"content\": prompt},\n",
    "               {\"role\": \"assistant\", \"content\": example['output'].strip()}]\n",
    "\n",
    "    prompt = student_tokenizer.apply_chat_template(message,\n",
    "                                                   tokenize=False,\n",
    "                                                   max_length=max_length,\n",
    "                                                   truncation=True)\n",
    "\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "formatted_dataset = formatted_dataset.map(format_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BerBNR4_MYi6",
    "outputId": "defc309a-6b4d-4d45-fa04-5faa174f6312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 27 Jul 2025\n",
      "\n",
      "You are a Medical Assistant follow the following instruction<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Choose the correct answer for the following question\n",
      "\n",
      "Entonox cylinder has blue body and white shoulder.\n",
      "\n",
      "Colour of Entonox cylinder is\n",
      "\n",
      "1) Black body, white shoulder.\n",
      "2) Grey body, black and white shoulder.\n",
      "3) Black body, brown and white shoulder.\n",
      "4) Blue body, white shoulder.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "4<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(formatted_dataset['prompt'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3wTd1ySJYrfn"
   },
   "outputs": [],
   "source": [
    "train_dataset = formatted_dataset\n",
    "eval_dataset = formatted_dataset.select(range(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sDkUJ8woY0TQ"
   },
   "outputs": [],
   "source": [
    "del formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVXu0ZplcqR1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 18:47:26.101300: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-28 18:47:26.109851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753728446.118059 3131151 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753728446.120796 3131151 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753728446.127975 3131151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753728446.127982 3131151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753728446.127983 3131151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753728446.127984 3131151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-28 18:47:26.129943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Training Arguments for Distillation\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation Trainer for Causal Language Models - FIXED VERSION\n",
    "class LlamaDistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, processing_class=None, **kwargs):\n",
    "        # Remove deprecated tokenizer parameter and use processing_class instead\n",
    "        if 'tokenizer' in kwargs:\n",
    "            processing_class = kwargs.pop('tokenizer')\n",
    "        \n",
    "        super().__init__(*args, processing_class=processing_class, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        if self.teacher_model:\n",
    "            self.teacher_model.eval()\n",
    "            # Ensure teacher model doesn't require gradients\n",
    "            for param in self.teacher_model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Ensure inputs have proper attention masks\n",
    "        if 'attention_mask' not in inputs:\n",
    "            inputs['attention_mask'] = (inputs['input_ids'] != self.processing_class.pad_token_id).long()\n",
    "        \n",
    "        # Get student outputs\n",
    "        outputs_student = model(**inputs)\n",
    "        loss_ce = outputs_student.loss\n",
    "        \n",
    "        # Handle case where loss might be None\n",
    "        if loss_ce is None:\n",
    "            # Manually compute cross-entropy loss\n",
    "            logits = outputs_student.logits\n",
    "            labels = inputs.get('labels', inputs['input_ids'])\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss_ce = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        logits_student = outputs_student.logits\n",
    "        \n",
    "        # Get teacher outputs (no gradients)\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                outputs_teacher = self.teacher_model(**inputs)\n",
    "                logits_teacher = outputs_teacher.logits\n",
    "            except Exception as e:\n",
    "                print(f\"Teacher model forward pass failed: {e}\")\n",
    "                # Fall back to student loss only\n",
    "                return (loss_ce, outputs_student) if return_outputs else loss_ce\n",
    "        \n",
    "        # Ensure same vocabulary size and sequence length\n",
    "        min_vocab_size = min(logits_student.size(-1), logits_teacher.size(-1))\n",
    "        min_seq_len = min(logits_student.size(1), logits_teacher.size(1))\n",
    "        \n",
    "        logits_student = logits_student[:, :min_seq_len, :min_vocab_size]\n",
    "        logits_teacher = logits_teacher[:, :min_seq_len, :min_vocab_size]\n",
    "        \n",
    "        # Compute distillation loss with proper error handling\n",
    "        try:\n",
    "            loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "            loss_kd = self.args.temperature ** 2 * loss_fct(\n",
    "                F.log_softmax(logits_student / self.args.temperature, dim=-1),\n",
    "                F.softmax(logits_teacher / self.args.temperature, dim=-1)\n",
    "            )\n",
    "            \n",
    "            # Check for NaN or infinite values\n",
    "            if torch.isnan(loss_kd) or torch.isinf(loss_kd):\n",
    "                print(\"Warning: KD loss is NaN or inf, using CE loss only\")\n",
    "                loss = loss_ce\n",
    "            else:\n",
    "                # Combine losses\n",
    "                loss = self.args.alpha * loss_ce + (1.0 - self.args.alpha) * loss_kd\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Distillation loss computation failed: {e}\")\n",
    "            loss = loss_ce\n",
    "        \n",
    "        # Final check for loss validity\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(\"Warning: Final loss is NaN or inf, skipping this batch\")\n",
    "            loss = torch.tensor(0.0, device=loss.device, requires_grad=True)\n",
    "        \n",
    "        return (loss, outputs_student) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_models_and_tokenizers():\n",
    "    \"\"\"Setup teacher and student models with tokenizers\"\"\"\n",
    "    \n",
    "    # Model checkpoints\n",
    "    teacher_ckpt = \"m42-health/Llama3-Med42-8B\"\n",
    "    student_ckpt = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    \n",
    "    print(\"Loading tokenizers...\")\n",
    "    # Load tokenizers (they should be compatible since both are Llama-based)\n",
    "    teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_ckpt)\n",
    "    student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
    "    \n",
    "    # Add padding token if not present\n",
    "    if teacher_tokenizer.pad_token is None:\n",
    "        teacher_tokenizer.pad_token = teacher_tokenizer.eos_token\n",
    "    if student_tokenizer.pad_token is None:\n",
    "        student_tokenizer.pad_token = student_tokenizer.eos_token\n",
    "    \n",
    "    print(\"Loading teacher model...\")\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                             bnb_4bit_use_double_quant=True,\n",
    "                                             bnb_4bit_quant_type=\"nf4\",\n",
    "                                             bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                             llm_int8_enable_fp32_cpu_offload=False)\n",
    "    \n",
    "    # Load teacher model\n",
    "    teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "        teacher_ckpt,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=torch.bfloat16,  # Use bfloat16 instead of float16\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,  # More memory efficient loading\n",
    "        use_cache=False,\n",
    "    )\n",
    "    \n",
    "    print(\"Loading student model...\")\n",
    "    # Load student model\n",
    "    student_model = AutoModelForCausalLM.from_pretrained(\n",
    "        student_ckpt,\n",
    "        torch_dtype=torch.bfloat16,  # Match teacher model dtype\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    \n",
    "    return teacher_model, student_model, teacher_tokenizer, student_tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, max_length=1024, text_column=\"prompt\"):\n",
    "    \"\"\"\n",
    "    Tokenize a dataset for training\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset object with text data\n",
    "        tokenizer: Tokenizer to use\n",
    "        max_length: Maximum sequence length\n",
    "        text_column: Name of the column containing text data\n",
    "    \n",
    "    Returns:\n",
    "        Tokenized dataset ready for training\n",
    "    \"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize the texts\n",
    "        tokenized = tokenizer(\n",
    "            examples[text_column],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Add labels for language modeling (same as input_ids)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "        return tokenized\n",
    "    \n",
    "    # Tokenize dataset with progress bar\n",
    "    print(f\"Tokenizing dataset with {len(dataset)} samples...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilled_model(\n",
    "    train_dataset, \n",
    "    eval_dataset=None, \n",
    "    tokenizer=None,\n",
    "    max_length=1024,\n",
    "    text_column=\"prompt\",\n",
    "    output_dir=\"./distil_med42_8B_Llama-3.2-1B-Instruct\",\n",
    "    num_epochs=1,\n",
    "    per_device_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-5,\n",
    "    alpha=0.7,\n",
    "    temperature=4.0,\n",
    "    resume_from_checkpoint=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Main training function with custom datasets\n",
    "    \n",
    "    Args:\n",
    "        train_dataset: Training dataset (Dataset, list of dicts, or pandas DataFrame)\n",
    "        eval_dataset: Evaluation dataset (Dataset, list of dicts, pandas DataFrame, or None)\n",
    "        tokenizer: Tokenizer to use (if None, will load student tokenizer)\n",
    "        max_length: Maximum sequence length\n",
    "        text_column: Name of the column containing text data\n",
    "        output_dir: Directory to save the model\n",
    "        num_epochs: Number of training epochs\n",
    "        per_device_batch_size: Batch size per device\n",
    "        gradient_accumulation_steps: Gradient accumulation steps\n",
    "        learning_rate: Learning rate\n",
    "        alpha: Distillation loss weight (0.0 = only KL loss, 1.0 = only CE loss)\n",
    "        temperature: Distillation temperature\n",
    "    \n",
    "    Returns:\n",
    "        Trained trainer object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup models and tokenizers\n",
    "    teacher_model, student_model, teacher_tokenizer, student_tokenizer = setup_models_and_tokenizers()\n",
    "    \n",
    "    # Use provided tokenizer or default to student tokenizer\n",
    "    if tokenizer is None:\n",
    "        tokenizer = student_tokenizer\n",
    "    \n",
    "    # Prepare datasets\n",
    "    print(\"Preparing custom datasets...\")\n",
    "\n",
    "    tokenized_train = tokenize_dataset(train_dataset, tokenizer, max_length, text_column)\n",
    "    # tokenized_eval = tokenize_dataset(eval_dataset, tokenizer, max_length, text_column)\n",
    "\n",
    "    # Calculate training steps\n",
    "    effective_batch_size = per_device_batch_size * gradient_accumulation_steps\n",
    "    total_steps = (len(tokenized_train) * num_epochs) // effective_batch_size\n",
    "\n",
    "    print(f\"Training configuration:\")\n",
    "    print(f\"  - Training samples: {len(tokenized_train):,}\")\n",
    "    # print(f\"  - Evaluation samples: {len(tokenized_eval) if tokenized_eval else 0:,}\")\n",
    "    print(f\"  - Per device batch size: {per_device_batch_size}\")\n",
    "    print(f\"  - Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "    print(f\"  - Effective batch size: {effective_batch_size}\")\n",
    "    print(f\"  - Number of epochs: {num_epochs}\")\n",
    "    print(f\"  - Total training steps: {total_steps:,}\")\n",
    "    print(f\"  - Alpha (CE weight): {alpha}\")\n",
    "    print(f\"  - Temperature: {temperature}\")\n",
    "    print(f\"  - Learning rate: {learning_rate}\")\n",
    "\n",
    "    training_args = DistillationTrainingArguments(\n",
    "        output_dir=\"./distil_med42_8B_Llama-3.2-1B-Instruct\",\n",
    "        run_name=\"llama-Instruct-distillation-v1\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=per_device_batch_size,\n",
    "        # per_device_eval_batch_size=per_device_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=5000,\n",
    "        save_steps=5000,\n",
    "        # eval_steps=5000,\n",
    "        # eval_strategy=\"steps\",\n",
    "        # load_best_model_at_end=True,\n",
    "        save_total_limit=1,\n",
    "        # metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=True,\n",
    "        max_grad_norm=1.0,\n",
    "        warmup_steps=1000,\n",
    "        dataloader_num_workers=4,\n",
    "        logging_dir=\"./logs\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        \n",
    "        push_to_hub=True,\n",
    "        hub_model_id=\"MohamedAhmedAE/distil_med42_8B_Llama-3.2-1B-Instruct\",\n",
    "        hub_strategy=\"checkpoint\",\n",
    "        hub_token=HF_API_KEY,\n",
    "        seed=2,\n",
    "        report_to='wandb',\n",
    "    )\n",
    "\n",
    "    trainer = LlamaDistillationTrainer(\n",
    "        model=student_model,\n",
    "        teacher_model=teacher_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        # eval_dataset=tokenized_eval,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Enable gradient checkpointing and disable cache\n",
    "    student_model.gradient_checkpointing_enable()\n",
    "    if hasattr(student_model.config, 'use_cache'):\n",
    "        student_model.config.use_cache = False\n",
    "    if hasattr(teacher_model.config, 'use_cache'):\n",
    "        teacher_model.config.use_cache = False\n",
    "    \n",
    "    print(\"Starting distillation training...\")\n",
    "    try:  \n",
    "        if resume_from_checkpoint is not None:\n",
    "            trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "        else:\n",
    "            trainer_stats = trainer.train()\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed with error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "    \n",
    "    return trainer, trainer_stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Llama Knowledge Distillation...\n",
      "Loading tokenizers...\n",
      "Loading teacher model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b626db63b54043b05e47a88574aab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading student model...\n",
      "Preparing custom datasets...\n",
      "Tokenizing dataset with 1437851 samples...\n",
      "Training configuration:\n",
      "  - Training samples: 1,437,851\n",
      "  - Per device batch size: 2\n",
      "  - Gradient accumulation steps: 1\n",
      "  - Effective batch size: 2\n",
      "  - Number of epochs: 1\n",
      "  - Total training steps: 718,925\n",
      "  - Alpha (CE weight): 0.7\n",
      "  - Temperature: 4.0\n",
      "  - Learning rate: 2e-05\n",
      "Starting distillation training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/maboelenen/Repos/ollama/wandb/run-20250728_184816-ksq16y2k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mohamed-ahmed/distil_med42_8B_Llama-3.2-1B-Instruct/runs/ksq16y2k' target=\"_blank\">llama-Instruct-distillation-v1</a></strong> to <a href='https://wandb.ai/mohamed-ahmed/distil_med42_8B_Llama-3.2-1B-Instruct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mohamed-ahmed/distil_med42_8B_Llama-3.2-1B-Instruct' target=\"_blank\">https://wandb.ai/mohamed-ahmed/distil_med42_8B_Llama-3.2-1B-Instruct</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mohamed-ahmed/distil_med42_8B_Llama-3.2-1B-Instruct/runs/ksq16y2k' target=\"_blank\">https://wandb.ai/mohamed-ahmed/distil_med42_8B_Llama-3.2-1B-Instruct/runs/ksq16y2k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45073' max='7189260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  45073/7189260 00:41 < 1152:06:41, 1.72 it/s, Epoch 0.06/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Starting Llama Knowledge Distillation...\")\n",
    "\n",
    "# Train the distilled model\n",
    "try:\n",
    "    trainer = train_distilled_model(train_dataset, eval_dataset, \n",
    "                                    max_length=1024,\n",
    "                                    per_device_batch_size=2,\n",
    "                                    resume_from_checkpoint=\"./distil_med42_8B_Llama-3.2-1B-Instruct/last-checkpoint\"\n",
    "                                   )\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    print(\"Proceeding with benchmarking existing models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Benchmark Class\n",
    "class PerformanceBenchmark:\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"Llama baseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "\n",
    "    def compute_accuracy(self):\n",
    "        \"\"\"\n",
    "        Compute accuracy for text generation tasks\n",
    "        Note: This is a simplified implementation. For proper evaluation,\n",
    "        you'd need task-specific metrics and ground truth labels.\n",
    "        \"\"\"\n",
    "        total_samples = len(self.dataset)\n",
    "        correct = 0\n",
    "        \n",
    "        for example in self.dataset[:10]:  # Sample subset for demo\n",
    "            try:\n",
    "                # Generate response\n",
    "                response = self.pipeline(\n",
    "                    example[\"text\"], \n",
    "                    max_length=50, \n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.pipeline.tokenizer.eos_token_id\n",
    "                )[0][\"generated_text\"]\n",
    "                # Simple check - if generation doesn't fail, count as \"correct\"\n",
    "                if len(response) > len(example[\"text\"]):\n",
    "                    correct += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Generation failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        accuracy = correct / min(10, total_samples) if total_samples > 0 else 0.0\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "    def compute_size(self):\n",
    "        \"\"\"Calculate model size in MB\"\"\"\n",
    "        state_dict = self.pipeline.model.state_dict()\n",
    "        tmp_path = Path(\"temp_model.pt\")\n",
    "        torch.save(state_dict, tmp_path)\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
    "        tmp_path.unlink()\n",
    "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "\n",
    "    def time_pipeline(self, query=\"What are the symptoms of diabetes?\"):\n",
    "        \"\"\"Measure inference latency\"\"\"\n",
    "        latencies = []\n",
    "        # Warmup\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                _ = self.pipeline(\n",
    "                    query, \n",
    "                    max_length=50, \n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.pipeline.tokenizer.eos_token_id\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Warmup failed: {e}\")\n",
    "                \n",
    "        # Timed run\n",
    "        for _ in range(10):\n",
    "            try:\n",
    "                start_time = perf_counter()\n",
    "                _ = self.pipeline(\n",
    "                    query, \n",
    "                    max_length=50, \n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.pipeline.tokenizer.eos_token_id\n",
    "                )\n",
    "                latency = perf_counter() - start_time\n",
    "                latencies.append(latency)\n",
    "            except Exception as e:\n",
    "                print(f\"Timing run failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if latencies:\n",
    "            time_avg_ms = 1000 * np.mean(latencies)\n",
    "            time_std_ms = 1000 * np.std(latencies)\n",
    "            print(f\"Average latency (ms) - {time_avg_ms:.2f} +/- {time_std_ms:.2f}\")\n",
    "            return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "        else:\n",
    "            print(\"No successful timing runs\")\n",
    "            return {\"time_avg_ms\": 0.0, \"time_std_ms\": 0.0}\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_models():\n",
    "    \"\"\"Benchmark the original and distilled models\"\"\"\n",
    "    \n",
    "    # Load models for benchmarking\n",
    "    print(\"Setting up benchmarking...\")\n",
    "    \n",
    "    # Original student model\n",
    "    try:\n",
    "        original_pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"meta-llama/Llama-3.2-1B\",\n",
    "            tokenizer=\"meta-llama/Llama-3.2-1B\",\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load original model: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    # Distilled model\n",
    "    try:\n",
    "        distilled_pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"./distil_med42_8B_Llama-3.2-1B-Instruct\",\n",
    "            tokenizer=\"./distil_med42_8B_Llama-3.2-1B-Instruct\",\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load distilled model: {e}\")\n",
    "        # Return only original model metrics\n",
    "        test_data = [\n",
    "            {\"prompt\": \"What are the symptoms of diabetes?\"},\n",
    "            {\"prompt\": \"How to treat high blood pressure?\"},\n",
    "            {\"prompt\": \"What causes heart disease?\"},\n",
    "        ]\n",
    "        perf_metrics = {}\n",
    "        pb_original = PerformanceBenchmark(original_pipe, test_data, \"Original Llama-3.2-1B\")\n",
    "        perf_metrics.update(pb_original.run_benchmark())\n",
    "        return perf_metrics\n",
    "    \n",
    "    # Create a simple test dataset\n",
    "    test_data = [\n",
    "        {\"prompt\": \"What are the symptoms of diabetes?\"},\n",
    "        {\"prompt\": \"How to treat high blood pressure?\"},\n",
    "        {\"prompt\": \"What causes heart disease?\"},\n",
    "    ]\n",
    "    \n",
    "    # Benchmark both models\n",
    "    perf_metrics = {}\n",
    "    \n",
    "    pb_original = PerformanceBenchmark(original_pipe, test_data, \"Original Llama-3.2-1B\")\n",
    "    perf_metrics.update(pb_original.run_benchmark())\n",
    "    \n",
    "    pb_distilled = PerformanceBenchmark(distilled_pipe, test_data, \"Distilled Llama-3.2-1B\")\n",
    "    perf_metrics.update(pb_distilled.run_benchmark())\n",
    "    \n",
    "    return perf_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(perf_metrics, current_optim_type):\n",
    "    \"\"\"Plot performance metrics\"\"\"\n",
    "    if not perf_metrics:\n",
    "        print(\"No metrics to plot\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient='index')\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        if idx == current_optim_type:\n",
    "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt.get(\"accuracy\", 0) * 100,\n",
    "                       alpha=0.7, s=df_opt[\"size_mb\"]/10, label=idx,\n",
    "                       marker='o', edgecolors='black', linewidth=2)\n",
    "        else:\n",
    "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt.get(\"accuracy\", 0) * 100,\n",
    "                       s=df_opt[\"size_mb\"]/10, label=idx, alpha=0.7)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Average latency (ms)\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Model Performance Comparison\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Benchmark the models\n",
    "# try:\n",
    "#     perf_metrics = benchmark_models()\n",
    "    \n",
    "#     # Plot results if we have metrics\n",
    "#     if perf_metrics:\n",
    "#         plot_metrics(perf_metrics, \"Distilled Llama-3.2-1B\")\n",
    "    \n",
    "#     print(\"Benchmarking complete!\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Benchmarking failed: {e}\")\n",
    "\n",
    "# print(\"Process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "060deed263cf463cbc7ebf82c3656b17": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0649221eb41b4979a48ec51b27f36541": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f5befc339c44977998ac3119277e32a",
      "placeholder": "​",
      "style": "IPY_MODEL_e7eefddd8caf46ab9e3197cd6a7234fb",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "09b78bd60e304b11b780b5ab215bf2a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f5befc339c44977998ac3119277e32a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "185a46dd0353472d9d9981ab04cfd091": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a2c9435870f4eaba68bfe7a6aaf7027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b7a53aa421246f48cbc74cfc8f2ebd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2883bdef3db14ef78d7d1488b12dba46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32dee9190d8545ab9afccd2e8e303740": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "477f507cbbef4fa9854f7ad6c2fcb235": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4822b16ce7d14a149d206730771f1cfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5c102a9cace84333bc4dfe38f241830b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe23bb3026c347c19fa396297ca9f8cd",
       "IPY_MODEL_7d88027b01504c6985b9d08feb0e7a36",
       "IPY_MODEL_ca0db323e8ca49a398195c12cafd829a"
      ],
      "layout": "IPY_MODEL_477f507cbbef4fa9854f7ad6c2fcb235"
     }
    },
    "5efa770141294af1a99a60db57da16f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66217af88b7942fb8a3997d27fc71a92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aa4c616893d493b8b74b4b0ac22bcb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d88027b01504c6985b9d08feb0e7a36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_060deed263cf463cbc7ebf82c3656b17",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1877b239e5649a89aeef94511ad9efd",
      "value": 1000
     }
    },
    "8a9ff03e8ed045e8a9c3b7c98e7eae47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e98afd2bf224341809af2057c359cad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2883bdef3db14ef78d7d1488b12dba46",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4822b16ce7d14a149d206730771f1cfa",
      "value": 50
     }
    },
    "a2448020b5be445faa8d6c554f68f49d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b05a68c6e0b549a18073088483944d13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1d047bc48d34c2a9435dc817720c1d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b22a41c2568f41feae2e9a932628bc4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0649221eb41b4979a48ec51b27f36541",
       "IPY_MODEL_c9bc5d7953724498910f012910f13f28",
       "IPY_MODEL_fd2a6a7ea0924fe5a54dbffdc99136a2"
      ],
      "layout": "IPY_MODEL_7aa4c616893d493b8b74b4b0ac22bcb2"
     }
    },
    "b447dd8f99ee4d00b89d8b9fcc21a1a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bfb6b3aaf98246f7ae63c0de70be2f5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09b78bd60e304b11b780b5ab215bf2a2",
      "placeholder": "​",
      "style": "IPY_MODEL_1a2c9435870f4eaba68bfe7a6aaf7027",
      "value": "Map: 100%"
     }
    },
    "c73b792c3aca4a52a88b42f426241206": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e033fa47a7cc442fbea4222579243514",
      "placeholder": "​",
      "style": "IPY_MODEL_32dee9190d8545ab9afccd2e8e303740",
      "value": " 50/50 [00:00&lt;00:00, 749.48 examples/s]"
     }
    },
    "c9bc5d7953724498910f012910f13f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b7a53aa421246f48cbc74cfc8f2ebd3",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b447dd8f99ee4d00b89d8b9fcc21a1a8",
      "value": 2
     }
    },
    "ca0db323e8ca49a398195c12cafd829a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2448020b5be445faa8d6c554f68f49d",
      "placeholder": "​",
      "style": "IPY_MODEL_185a46dd0353472d9d9981ab04cfd091",
      "value": " 1000/1000 [00:00&lt;00:00, 1371.13 examples/s]"
     }
    },
    "e033fa47a7cc442fbea4222579243514": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1877b239e5649a89aeef94511ad9efd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e7eefddd8caf46ab9e3197cd6a7234fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd2a6a7ea0924fe5a54dbffdc99136a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1d047bc48d34c2a9435dc817720c1d3",
      "placeholder": "​",
      "style": "IPY_MODEL_5efa770141294af1a99a60db57da16f1",
      "value": " 2/2 [00:35&lt;00:00, 17.27s/it]"
     }
    },
    "fd2fe620183a49879f3cb3547616d40e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bfb6b3aaf98246f7ae63c0de70be2f5b",
       "IPY_MODEL_9e98afd2bf224341809af2057c359cad",
       "IPY_MODEL_c73b792c3aca4a52a88b42f426241206"
      ],
      "layout": "IPY_MODEL_b05a68c6e0b549a18073088483944d13"
     }
    },
    "fe23bb3026c347c19fa396297ca9f8cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66217af88b7942fb8a3997d27fc71a92",
      "placeholder": "​",
      "style": "IPY_MODEL_8a9ff03e8ed045e8a9c3b7c98e7eae47",
      "value": "Map: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
